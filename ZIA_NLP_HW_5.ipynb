{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №5.  Part-of-Speech разметка, NER, извлечение отношений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download() \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#для задания 1\n",
    "import pyconll\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#для задания 2\n",
    "import os\n",
    "import corus\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy info\n",
    "#!python -m spacy download ru_core_news_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import ru_core_news_sm\n",
    "#!pip install slovnet\n",
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from razdel import tokenize\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "**Задание 1.** Написать теггер на данных с русским языком\n",
    "\n",
    "-- проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации;\n",
    "\n",
    "-- написать свой теггер как на занятии (попробовать разные векторайзеры, добавить знание не только букв, но и слов);\n",
    "\n",
    "-- сравнить все реализованные методы сделать выводы.\n",
    " \n",
    "**Задание 2.** Проверить насколько хорошо работает NER\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "\n",
    "-- проверить NER из nltk/spacy/deeppavlov\n",
    "\n",
    "-- написать свой нер попробовать разные подходы:\n",
    "\n",
    "передаём в сетку токен и его соседей\n",
    "\n",
    "передаём в сетку только токен\n",
    "\n",
    "-- сравнить ваши реализованные подходы на качество (вывести precision/recall/f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План решения\n",
    "\n",
    "[1. POS tagging на русскоязычном корпусе](#section_1)\n",
    "\n",
    "[1.1. Теггеры UnigramTagger, BigramTagger, TrigramTagger и их комбмнации](#section_1.1)\n",
    "\n",
    "[1.2. Самописный теггер с различными векторайзерами](#section_1.2)\n",
    "\n",
    "[1.3. Выводы](#section_1.3)\n",
    "\n",
    "[2. Named-entity recognition (NER)](#section_2)\n",
    "\n",
    "[2.1. NER из NLTK](#section_2.1)\n",
    "\n",
    "[2.2. NER из Spacy](#section_2.2)\n",
    "\n",
    "[2.3. Ner из slovnet](#section_2.3)\n",
    "\n",
    "[2.4. Самописный NER](#section_2.4)\n",
    "\n",
    "[2.5. Выводы](#section_2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. POS tagging на русскоязычном корпусе  <a id='section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные были предварительно скачены на локальный компьютер с https://github.com/UniversalDependencies/UD_Russian-SynTagRus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('ru_syntagrus-ud-train-a.conllu')\n",
    "full_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просматриваем 2 предложения тренировочных данных:\n",
    "#в каждом из них для каждого токена выводим form (словоформа для чтения) и upos (часть речи)\n",
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 194\n",
      "Наибольшая длина токена 31\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24516, 8906, 8906)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Анкета', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Начальник', 'NOUN'),\n",
       "  ('областного', 'ADJ'),\n",
       "  ('управления', 'NOUN'),\n",
       "  ('связи', 'NOUN'),\n",
       "  ('Семен', 'PROPN'),\n",
       "  ('Еремеевич', 'PROPN'),\n",
       "  ('был', 'AUX'),\n",
       "  ('человек', 'NOUN'),\n",
       "  ('простой', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('приходил', 'VERB'),\n",
       "  ('на', 'ADP'),\n",
       "  ('работу', 'NOUN'),\n",
       "  ('всегда', 'ADV'),\n",
       "  ('вовремя', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('здоровался', 'VERB'),\n",
       "  ('с', 'ADP'),\n",
       "  ('секретаршей', 'NOUN'),\n",
       "  ('за', 'ADP'),\n",
       "  ('руку', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('иногда', 'ADV'),\n",
       "  ('даже', 'PART'),\n",
       "  ('писал', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('стенгазету', 'NOUN'),\n",
       "  ('заметки', 'NOUN'),\n",
       "  ('под', 'ADP'),\n",
       "  ('псевдонимом', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('Муха', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('В', 'ADP'),\n",
       "  ('приемной', 'NOUN'),\n",
       "  ('его', 'PRON'),\n",
       "  ('с', 'ADP'),\n",
       "  ('утра', 'NOUN'),\n",
       "  ('ожидали', 'VERB'),\n",
       "  ('посетители', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('-', 'PUNCT'),\n",
       "  ('кое-кто', 'PRON'),\n",
       "  ('с', 'ADP'),\n",
       "  ('важными', 'ADJ'),\n",
       "  ('делами', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('а', 'CCONJ'),\n",
       "  ('кое-кто', 'PRON'),\n",
       "  ('и', 'PART'),\n",
       "  ('с', 'ADP'),\n",
       "  ('такими', 'DET'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('которые', 'PRON'),\n",
       "  ('легко', 'ADV'),\n",
       "  ('можно', 'ADV'),\n",
       "  ('было', 'AUX'),\n",
       "  ('решить', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('нижестоящих', 'ADJ'),\n",
       "  ('инстанциях', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('не', 'PART'),\n",
       "  ('затрудняя', 'VERB'),\n",
       "  ('Семена', 'PROPN'),\n",
       "  ('Еремеевича', 'PROPN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Однако', 'ADV'),\n",
       "  ('стиль', 'NOUN'),\n",
       "  ('работы', 'NOUN'),\n",
       "  ('Семена', 'PROPN'),\n",
       "  ('Еремеевича', 'PROPN'),\n",
       "  ('заключался', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('том', 'PRON'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('чтобы', 'SCONJ'),\n",
       "  ('принимать', 'VERB'),\n",
       "  ('всех', 'DET'),\n",
       "  ('желающих', 'VERB'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('лично', 'ADV'),\n",
       "  ('вникать', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('дело', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Приемная', 'NOUN'),\n",
       "  ('была', 'AUX'),\n",
       "  ('обставлена', 'VERB'),\n",
       "  ('просто', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('но', 'CCONJ'),\n",
       "  ('по-деловому', 'ADV'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#просматриваем тренировочные данные (токен, тег)\n",
    "fdata_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Теггеры UnigramTagger, BigramTagger, TrigramTagger и их комбмнации <a id='section_1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UnigramTagger** (\n",
    "UnigramTagger учитывает условную частоту тегов и предсказывает наиболее частый тег для каждого токена, не ориентируется на соседние слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.823732013802982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "accuracy_U = unigram_tagger.evaluate(fdata_test)\n",
    "display(accuracy_U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BigramTagger** (BigramTagger учитывает тэги двух слов: текущее и предыдущее слово)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6093886320724006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "accuracy_B = bigram_tagger.evaluate(fdata_test)\n",
    "display(accuracy_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TrigramTagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1778631421316492"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "accuracy_T = trigram_tagger.evaluate(fdata_test)\n",
    "display(accuracy_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комбинация теггеров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8275343446838986, 0.8273650628296113, 0.1750309264926102, 0.827905462595221]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1 = [UnigramTagger, BigramTagger]\n",
    "\n",
    "list_2 = [UnigramTagger, TrigramTagger]\n",
    "\n",
    "list_3 = [BigramTagger, TrigramTagger]\n",
    "\n",
    "list_4 = [UnigramTagger, BigramTagger, TrigramTagger]\n",
    "\n",
    "accuracy_N = []\n",
    "\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "for list_N in [list_1, list_2, list_3, list_4]:\n",
    "    backoff = DefaultTagger('NN') \n",
    "    tag = backoff_tagger(fdata_train,  \n",
    "                         list_N,  \n",
    "                         backoff = backoff) \n",
    "\n",
    "    accuracy_N.append(tag.evaluate(fdata_test))\n",
    "\n",
    "accuracy_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tagger</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnigramTagger+BigramTagger+TrigramTagger</td>\n",
       "      <td>0.827905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unigram + Bigram</td>\n",
       "      <td>0.827534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnigramTagger + TrigramTagger</td>\n",
       "      <td>0.827365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unigram</td>\n",
       "      <td>0.823732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bigram</td>\n",
       "      <td>0.609389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trigram</td>\n",
       "      <td>0.177863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BigramTagger + TrigramTagger</td>\n",
       "      <td>0.175031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tagger  Accuracy\n",
       "6  UnigramTagger+BigramTagger+TrigramTagger  0.827905\n",
       "3                          Unigram + Bigram  0.827534\n",
       "4             UnigramTagger + TrigramTagger  0.827365\n",
       "0                                   Unigram  0.823732\n",
       "1                                    Bigram  0.609389\n",
       "2                                   Trigram  0.177863\n",
       "5              BigramTagger + TrigramTagger  0.175031"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({'Tagger': ['Unigram', 'Bigram', 'Trigram', 'Unigram + Bigram', 'UnigramTagger + TrigramTagger', 'BigramTagger + TrigramTagger', 'UnigramTagger+BigramTagger+TrigramTagger'], 'Accuracy' : [accuracy_U, accuracy_B, accuracy_T] + accuracy_N})\n",
    "result.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** лучшие показатели метрики у комбинации теггеров UnigramTagger+BigramTagger+TrigramTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Самописный теггер с различными векторайзерами <a id='section_1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем токены и лэйблы в списки\n",
    "train_tok = [] #список токенов\n",
    "train_label = [] #список теггов\n",
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#кодируем целевые метки значениями от 0 до (кол-во классов - 1)\n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.91      0.92     11247\n",
      "         ADP       0.98      1.00      0.99     10255\n",
      "         ADV       0.92      0.90      0.91      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.98      0.93      4276\n",
      "         DET       0.88      0.75      0.81      2978\n",
      "        INTJ       0.33      0.36      0.35        11\n",
      "        NOUN       0.92      0.95      0.94     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.86      0.90      0.88      1436\n",
      "        PART       0.95      0.78      0.86      3762\n",
      "        PRON       0.83      0.89      0.86      5346\n",
      "       PROPN       0.79      0.59      0.67      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.91      0.86      2176\n",
      "         SYM       1.00      0.68      0.81        53\n",
      "        VERB       0.94      0.94      0.94     12617\n",
      "           X       0.47      0.16      0.24       105\n",
      "\n",
      "    accuracy                           0.93    115000\n",
      "   macro avg       0.85      0.82      0.82    115000\n",
      "weighted avg       0.93      0.93      0.93    115000\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.90      0.91      0.91     11247\n",
      "         ADP       0.99      0.99      0.99     10255\n",
      "         ADV       0.92      0.86      0.89      5986\n",
      "         AUX       0.81      0.97      0.89      1058\n",
      "       CCONJ       0.88      0.98      0.93      4276\n",
      "         DET       0.80      0.83      0.82      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.90      0.96      0.93     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.85      0.90      0.87      1436\n",
      "        PART       0.95      0.79      0.86      3762\n",
      "        PRON       0.87      0.84      0.86      5346\n",
      "       PROPN       0.80      0.52      0.63      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.91      0.86      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.93      0.93      0.93     12617\n",
      "           X       0.45      0.09      0.14       105\n",
      "\n",
      "    accuracy                           0.92    115000\n",
      "   macro avg       0.83      0.78      0.79    115000\n",
      "weighted avg       0.92      0.92      0.92    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.84      0.84      0.84     11247\n",
      "         ADP       0.97      0.98      0.98     10255\n",
      "         ADV       0.83      0.79      0.81      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.80      0.80      0.80      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.84      0.90      0.87     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.81      0.82      0.82      1436\n",
      "        PART       0.92      0.78      0.84      3762\n",
      "        PRON       0.84      0.86      0.85      5346\n",
      "       PROPN       0.72      0.45      0.55      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.88      0.84      0.86     12617\n",
      "           X       0.31      0.05      0.08       105\n",
      "\n",
      "    accuracy                           0.89    115000\n",
      "   macro avg       0.79      0.75      0.76    115000\n",
      "weighted avg       0.88      0.89      0.88    115000\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.95      0.44      0.60     11247\n",
      "         ADP       0.99      0.48      0.65     10255\n",
      "         ADV       0.93      0.78      0.85      5986\n",
      "         AUX       0.83      0.88      0.85      1058\n",
      "       CCONJ       0.90      0.20      0.33      4276\n",
      "         DET       0.84      0.66      0.74      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.98      0.68      0.80     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.88      0.52      0.65      1436\n",
      "        PART       0.96      0.75      0.84      3762\n",
      "        PRON       0.83      0.80      0.82      5346\n",
      "       PROPN       0.96      0.13      0.23      4315\n",
      "       PUNCT       0.37      1.00      0.54     21941\n",
      "       SCONJ       0.72      0.83      0.77      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.97      0.46      0.62     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.65    115000\n",
      "   macro avg       0.67      0.48      0.52    115000\n",
      "weighted avg       0.83      0.65      0.66    115000\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.95      0.44      0.60     11247\n",
      "         ADP       0.99      0.48      0.65     10255\n",
      "         ADV       0.96      0.79      0.87      5986\n",
      "         AUX       0.83      0.88      0.85      1058\n",
      "       CCONJ       0.89      0.20      0.33      4276\n",
      "         DET       0.84      0.72      0.77      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.98      0.68      0.80     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.89      0.55      0.68      1436\n",
      "        PART       0.97      0.75      0.84      3762\n",
      "        PRON       0.83      0.84      0.83      5346\n",
      "       PROPN       0.94      0.14      0.24      4315\n",
      "       PUNCT       0.38      1.00      0.55     21941\n",
      "       SCONJ       0.79      0.85      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.97      0.46      0.62     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.65    115000\n",
      "   macro avg       0.68      0.49      0.53    115000\n",
      "weighted avg       0.84      0.65      0.66    115000\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.43      0.21      0.28     11247\n",
      "         ADP       0.83      0.47      0.60     10255\n",
      "         ADV       0.59      0.63      0.61      5986\n",
      "         AUX       0.70      0.94      0.80      1058\n",
      "       CCONJ       0.84      0.18      0.30      4276\n",
      "         DET       0.49      0.55      0.52      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.25      0.53      0.34     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.41      0.43      0.42      1436\n",
      "        PART       0.86      0.76      0.81      3762\n",
      "        PRON       0.64      0.76      0.70      5346\n",
      "       PROPN       0.32      0.08      0.13      4315\n",
      "       PUNCT       0.00      0.00      0.00     21941\n",
      "       SCONJ       0.67      0.95      0.78      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.45      0.25      0.32     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.37    115000\n",
      "   macro avg       0.42      0.38      0.37    115000\n",
      "weighted avg       0.39      0.37      0.35    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.87      0.82      0.84      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.84      0.75      0.80      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.86      0.93      0.89     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.83      0.84      0.83      1436\n",
      "        PART       0.94      0.78      0.85      3762\n",
      "        PRON       0.83      0.89      0.85      5346\n",
      "       PROPN       0.74      0.41      0.53      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.89      0.88      0.89     12617\n",
      "           X       0.38      0.06      0.10       105\n",
      "\n",
      "    accuracy                           0.90    115000\n",
      "   macro avg       0.81      0.76      0.77    115000\n",
      "weighted avg       0.90      0.90      0.90    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.88      0.82      0.85      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.98      0.93      4276\n",
      "         DET       0.88      0.72      0.79      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.87      0.93      0.90     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.82      0.84      0.83      1436\n",
      "        PART       0.95      0.77      0.85      3762\n",
      "        PRON       0.82      0.91      0.86      5346\n",
      "       PROPN       0.74      0.40      0.52      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.79      0.88        53\n",
      "        VERB       0.89      0.88      0.89     12617\n",
      "           X       0.26      0.10      0.14       105\n",
      "\n",
      "    accuracy                           0.90    115000\n",
      "   macro avg       0.80      0.77      0.78    115000\n",
      "weighted avg       0.90      0.90      0.90    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.88      0.88     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.90      0.83      0.86      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.83      0.78      0.81      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.87      0.94      0.90     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.84      0.87      0.85      1436\n",
      "        PART       0.94      0.78      0.85      3762\n",
      "        PRON       0.83      0.86      0.85      5346\n",
      "       PROPN       0.76      0.41      0.54      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.89      0.90      0.89     12617\n",
      "           X       0.50      0.05      0.09       105\n",
      "\n",
      "    accuracy                           0.91    115000\n",
      "   macro avg       0.82      0.77      0.77    115000\n",
      "weighted avg       0.90      0.91      0.90    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=10000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.88      0.88     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.90      0.84      0.87      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.83      0.78      0.81      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.88      0.94      0.91     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.84      0.88      0.86      1436\n",
      "        PART       0.94      0.79      0.85      3762\n",
      "        PRON       0.82      0.87      0.85      5346\n",
      "       PROPN       0.78      0.42      0.54      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.82      0.86      0.84      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.90      0.90      0.90     12617\n",
      "           X       0.71      0.05      0.09       105\n",
      "\n",
      "    accuracy                           0.91    115000\n",
      "   macro avg       0.83      0.77      0.78    115000\n",
      "weighted avg       0.91      0.91      0.90    115000\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.50      0.26      0.34     11247\n",
      "         ADP       0.90      0.48      0.62     10255\n",
      "         ADV       0.68      0.69      0.68      5986\n",
      "         AUX       0.75      0.94      0.84      1058\n",
      "       CCONJ       0.89      0.18      0.31      4276\n",
      "         DET       0.67      0.53      0.59      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.61      0.58      0.59     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.53      0.47      0.50      1436\n",
      "        PART       0.91      0.76      0.83      3762\n",
      "        PRON       0.69      0.85      0.76      5346\n",
      "       PROPN       0.39      0.10      0.15      4315\n",
      "       PUNCT       0.48      1.00      0.65     21941\n",
      "       SCONJ       0.76      0.90      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.55      0.31      0.39     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.58    115000\n",
      "   macro avg       0.52      0.45      0.45    115000\n",
      "weighted avg       0.62      0.58      0.56    115000\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.55      0.30      0.39     11247\n",
      "         ADP       0.92      0.48      0.63     10255\n",
      "         ADV       0.77      0.71      0.74      5986\n",
      "         AUX       0.77      0.94      0.85      1058\n",
      "       CCONJ       0.93      0.18      0.30      4276\n",
      "         DET       0.71      0.57      0.63      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.65      0.60      0.62     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.61      0.49      0.54      1436\n",
      "        PART       0.90      0.78      0.84      3762\n",
      "        PRON       0.74      0.83      0.79      5346\n",
      "       PROPN       0.44      0.12      0.18      4315\n",
      "       PUNCT       0.47      1.00      0.64     21941\n",
      "       SCONJ       0.73      0.95      0.83      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.60      0.33      0.43     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.60    115000\n",
      "   macro avg       0.54      0.46      0.47    115000\n",
      "weighted avg       0.65      0.60      0.58    115000\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.63      0.33      0.44     11247\n",
      "         ADP       0.91      0.48      0.63     10255\n",
      "         ADV       0.82      0.72      0.77      5986\n",
      "         AUX       0.79      0.95      0.86      1058\n",
      "       CCONJ       0.92      0.19      0.31      4276\n",
      "         DET       0.69      0.70      0.70      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.70      0.62      0.65     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.66      0.48      0.55      1436\n",
      "        PART       0.91      0.76      0.83      3762\n",
      "        PRON       0.79      0.79      0.79      5346\n",
      "       PROPN       0.54      0.13      0.21      4315\n",
      "       PUNCT       0.45      1.00      0.62     21941\n",
      "       SCONJ       0.76      0.88      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.67      0.36      0.47     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.61    115000\n",
      "   macro avg       0.57      0.47      0.48    115000\n",
      "weighted avg       0.68      0.61      0.59    115000\n",
      "\n",
      "HashingVectorizer(n_features=10000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.72      0.36      0.48     11247\n",
      "         ADP       0.96      0.48      0.64     10255\n",
      "         ADV       0.88      0.75      0.81      5986\n",
      "         AUX       0.81      0.88      0.84      1058\n",
      "       CCONJ       0.88      0.20      0.33      4276\n",
      "         DET       0.72      0.75      0.74      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.77      0.64      0.70     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.74      0.58      0.65      1436\n",
      "        PART       0.96      0.75      0.84      3762\n",
      "        PRON       0.83      0.78      0.81      5346\n",
      "       PROPN       0.67      0.15      0.24      4315\n",
      "       PUNCT       0.42      1.00      0.60     21941\n",
      "       SCONJ       0.78      0.88      0.83      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.75      0.41      0.53     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.63    115000\n",
      "   macro avg       0.60      0.48      0.50    115000\n",
      "weighted avg       0.72      0.63      0.62    115000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#строим модели логистической регрессии с различными векторайзерами\n",
    "#P.S. тестовые данные срезаны до 115000 штук из-за битых файлов\n",
    "\n",
    "vectorizers = [CountVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000, 10000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok[:115000])\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels[:115000], pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels[:115000], pred)\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels[:115000], pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.927836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.921158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.903654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.901213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.897075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.895273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.882215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.662784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.658415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer(n_features=10000, ngram_rang...</td>\n",
       "      <td>0.620760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.594828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.555762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.345039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  f1_score\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.927836\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.921158\n",
       "9   HashingVectorizer(analyzer='char', n_features=...  0.903654\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.901213\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.897075\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.895273\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.882215\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.662784\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.658415\n",
       "13  HashingVectorizer(n_features=10000, ngram_rang...  0.620760\n",
       "12  HashingVectorizer(n_features=5000, ngram_range...  0.594828\n",
       "11  HashingVectorizer(n_features=3000, ngram_range...  0.577113\n",
       "10  HashingVectorizer(n_features=2000, ngram_range...  0.555762\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.345039"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'f1_score': f1_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.929513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.923583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.907417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.905009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.900913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.898939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.885157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.653252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.649052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer(n_features=10000, ngram_rang...</td>\n",
       "      <td>0.630539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.612913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.601191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.584009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.365243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  Accuracy\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.929513\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.923583\n",
       "9   HashingVectorizer(analyzer='char', n_features=...  0.907417\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.905009\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.900913\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.898939\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.885157\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.653252\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.649052\n",
       "13  HashingVectorizer(n_features=10000, ngram_rang...  0.630539\n",
       "12  HashingVectorizer(n_features=5000, ngram_range...  0.612913\n",
       "11  HashingVectorizer(n_features=3000, ngram_range...  0.601191\n",
       "10  HashingVectorizer(n_features=2000, ngram_range...  0.584009\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.365243"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'Accuracy': accuracy_scores})\n",
    "result_model_acc.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** лучшие показатели метрик f1 и accuracy у CountVectorizer(analyzer='char', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Выводы <a id='section_1.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tagger</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UnigramTagger+BigramTagger+TrigramTagger</td>\n",
       "      <td>0.827905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tagger  Accuracy\n",
       "6  UnigramTagger+BigramTagger+TrigramTagger  0.827905"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values('Accuracy', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.927836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vectorizer  f1_score\n",
       "0  CountVectorizer(analyzer='char', ngram_range=(...  0.927836"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model.sort_values('f1_score', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.929513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vectorizer  Accuracy\n",
       "0  CountVectorizer(analyzer='char', ngram_range=(...  0.929513"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_acc.sort_values('Accuracy', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** лучший метод pos tagging (по результатам подсчета метрик) -- самописный теггер с CountVectorizer(analyzer='char', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Named-entity recognition (NER) <a id='section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу извлечения именованных сущностей на основе размеченной коллекции http://www.labinform.ru/pub/named_entities/collection3.\n",
    "\n",
    "Датасет содержит 1000 файлов txt и 1000 файлов ann, в которых размечены 3 сущности:\n",
    " - имена людей (PER),\n",
    " - имена организаций (ORG),\n",
    " - географические названия (LOC). \n",
    " \n",
    " P.S. Все файлы предварительно загружены на локальный компьютер; файлы 595.txt и 595.ann битые, в следcтвие чего были удалены из рассмотрения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверим работу NER из nltk, NER из spacy и NER из slovnet на указанной коллекции**\n",
    "\n",
    "**Важно:** загрузка файлов через load_ne5(dir) невозможна из-за ошибки кодировки при открытии файлов (UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 171: character maps to <undefined>)\n",
    "\n",
    "P.S. deeppavlov не рассматривается в виду отсутствия необходимых вычислительных мощностей для его установки (надо переустанавливать tensorflow на более раннюю версию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.ann', '001.txt', '002.ann', '002.txt', '003.ann', '003.txt', '004.ann', '004.txt', '005.ann', '005.txt', '006.ann', '006.txt', '007.ann', '007.txt', '008.ann', '008.txt', '009.ann', '009.txt', '010.ann', '010.txt', '011.ann', '011.txt', '012.ann', '012.txt', '013.ann', '013.txt', '014.ann', '014.txt', '015 (!).ann', '015 (!).txt', '016.ann', '016.txt', '017.ann', '017.txt', '018.ann', '018.txt', '019.ann', '019.txt', '020.ann', '020.txt', '021.ann', '021.txt', '022.ann', '022.txt', '023.ann', '023.txt', '025.ann', '025.txt', '026.ann', '026.txt', '027.ann', '027.txt', '028.ann', '028.txt', '029.ann', '029.txt', '030.ann', '030.txt', '031.ann', '031.txt', '032.ann', '032.txt', '033.ann', '033.txt', '034.ann', '034.txt', '035.ann', '035.txt', '036.ann', '036.txt', '037.ann', '037.txt', '038.ann', '038.txt', '039.ann', '039.txt', '03_12_12a.ann', '03_12_12a.txt', '03_12_12b.ann', '03_12_12b.txt', '03_12_12c.ann', '03_12_12c.txt', '03_12_12d.ann', '03_12_12d.txt', '03_12_12g.ann', '03_12_12g.txt', '03_12_12h.ann', '03_12_12h.txt', '040.ann', '040.txt', '041.ann', '041.txt', '042.ann', '042.txt', '043.ann', '043.txt', '044.ann', '044.txt', '045.ann', '045.txt', '046.ann', '046.txt', '047.ann', '047.txt', '048.ann', '048.txt', '049.ann', '049.txt', '04_02_13a_abdulatipov.ann', '04_02_13a_abdulatipov.txt', '04_03_13a_sorokin.ann', '04_03_13a_sorokin.txt', '04_12_12b.ann', '04_12_12b.txt', '04_12_12d.ann', '04_12_12d.txt', '04_12_12f.ann', '04_12_12f.txt', '04_12_12g.ann', '04_12_12g.txt', '04_12_12h_corr.ann', '04_12_12h_corr.txt', '050.ann', '050.txt', '051.ann', '051.txt', '052.ann', '052.txt', '053.ann', '053.txt', '054.ann', '054.txt', '055.ann', '055.txt', '056.ann', '056.txt', '057.ann', '057.txt', '058.ann', '058.txt', '059.ann', '059.txt', '060.ann', '060.txt', '061.ann', '061.txt', '062.ann', '062.txt', '063.ann', '063.txt', '064.ann', '064.txt', '065.ann', '065.txt', '066.ann', '066.txt', '067.ann', '067.txt', '068.ann', '068.txt', '069.ann', '069.txt', '070.ann', '070.txt', '071.ann', '071.txt', '072.ann', '072.txt', '073.ann', '073.txt', '074.ann', '074.txt', '075.ann', '075.txt', '076.ann', '076.txt', '077.ann', '077.txt', '078.ann', '078.txt', '079.ann', '079.txt', '080.ann', '080.txt', '081.ann', '081.txt', '082.ann', '082.txt', '083.ann', '083.txt', '084.ann', '084.txt', '085.ann', '085.txt', '086.ann', '086.txt', '087.ann', '087.txt', '088.ann', '088.txt', '089.ann', '089.txt', '090.ann', '090.txt', '091.ann', '091.txt', '092.ann', '092.txt', '093.ann', '093.txt', '094.ann', '094.txt', '095.ann', '095.txt', '096.ann', '096.txt', '097.ann', '097.txt', '098.ann', '098.txt', '099.ann', '099.txt', '09_01_13.ann', '09_01_13.txt', '09_01_13a.ann', '09_01_13a.txt', '09_01_13c.ann', '09_01_13c.txt', '09_01_13d.ann', '09_01_13d.txt', '09_01_13e.ann', '09_01_13e.txt', '09_01_13h.ann', '09_01_13h.txt', '09_01_13i.ann', '09_01_13i.txt', '100.ann', '100.txt', '1000.ann', '1000.txt', '1001.ann', '1001.txt', '1002.ann', '1002.txt', '1003.ann', '1003.txt', '1004.ann', '1004.txt', '1005.ann', '1005.txt', '1006.ann', '1006.txt', '1007.ann', '1007.txt', '1008.ann', '1008.txt', '1009.ann', '1009.txt', '101.ann', '101.txt', '1010.ann', '1010.txt', '1011.ann', '1011.txt', '1012.ann', '1012.txt', '1013.ann', '1013.txt', '1014.ann', '1014.txt', '1015.ann', '1015.txt', '1016.ann', '1016.txt', '1017.ann', '1017.txt', '1018.ann', '1018.txt', '1019.ann', '1019.txt', '102.ann', '102.txt', '1020.ann', '1020.txt', '1021.ann', '1021.txt', '1022.ann', '1022.txt', '1023.ann', '1023.txt', '1024.ann', '1024.txt', '1025.ann', '1025.txt', '1026.ann', '1026.txt', '1027.ann', '1027.txt', '1028.ann', '1028.txt', '1029.ann', '1029.txt', '103.ann', '103.txt', '1030.ann', '1030.txt', '1031.ann', '1031.txt', '1032.ann', '1032.txt', '1033.ann', '1033.txt', '1034.ann', '1034.txt', '1035.ann', '1035.txt', '1036.ann', '1036.txt', '1037.ann', '1037.txt', '1038.ann', '1038.txt', '1039.ann', '1039.txt', '104.ann', '104.txt', '1040.ann', '1040.txt', '1041.ann', '1041.txt', '1042.ann', '1042.txt', '1043.ann', '1043.txt', '1044.ann', '1044.txt', '1045.ann', '1045.txt', '1046.ann', '1046.txt', '1047.ann', '1047.txt', '1048.ann', '1048.txt', '1049.ann', '1049.txt', '105.ann', '105.txt', '1050.ann', '1050.txt', '106.ann', '106.txt', '107.ann', '107.txt', '108.ann', '108.txt', '109.ann', '109.txt', '10_01_13a.ann', '10_01_13a.txt', '10_01_13d.ann', '10_01_13d.txt', '10_01_13i.ann', '10_01_13i.txt', '110.ann', '110.txt', '1100.ann', '1100.txt', '1101.ann', '1101.txt', '1102.ann', '1102.txt', '1103.ann', '1103.txt', '1104.ann', '1104.txt', '1105.ann', '1105.txt', '1106.ann', '1106.txt', '1107.ann', '1107.txt', '1108.ann', '1108.txt', '1109.ann', '1109.txt', '111.ann', '111.txt', '1110.ann', '1110.txt', '1111.ann', '1111.txt', '1112.ann', '1112.txt', '1113.ann', '1113.txt', '1114.ann', '1114.txt', '1115.ann', '1115.txt', '1116.ann', '1116.txt', '1117.ann', '1117.txt', '1118.ann', '1118.txt', '1119.ann', '1119.txt', '112.ann', '112.txt', '1120.ann', '1120.txt', '1121.ann', '1121.txt', '1122.ann', '1122.txt', '1123.ann', '1123.txt', '1124.ann', '1124.txt', '1125.ann', '1125.txt', '1126.ann', '1126.txt', '1127.ann', '1127.txt', '1128.ann', '1128.txt', '113.ann', '113.txt', '1130.ann', '1130.txt', '1131.ann', '1131.txt', '1132.ann', '1132.txt', '1133.ann', '1133.txt', '1134.ann', '1134.txt', '1135.ann', '1135.txt', '1136.ann', '1136.txt', '1137.ann', '1137.txt', '1138.ann', '1138.txt', '1139.ann', '1139.txt', '114.ann', '114.txt', '1140.ann', '1140.txt', '1141.ann', '1141.txt', '1142.ann', '1142.txt', '1143.ann', '1143.txt', '1144.ann', '1144.txt', '1145.ann', '1145.txt', '1146.ann', '1146.txt', '1147.ann', '1147.txt', '1148.ann', '1148.txt', '1149.ann', '1149.txt', '115.ann', '115.txt', '1150.ann', '1150.txt', '1151.ann', '1151.txt', '1152.ann', '1152.txt', '1153.ann', '1153.txt', '1154.ann', '1154.txt', '1155.ann', '1155.txt', '1156.ann', '1156.txt', '1157.ann', '1157.txt', '1158.ann', '1158.txt', '1159.ann', '1159.txt', '116.ann', '116.txt', '1160.ann', '1160.txt', '1161.ann', '1161.txt', '1162.ann', '1162.txt', '1163.ann', '1163.txt', '1164.ann', '1164.txt', '1165.ann', '1165.txt', '1166.ann', '1166.txt', '1167.ann', '1167.txt', '1168.ann', '1168.txt', '1169.ann', '1169.txt', '117.ann', '117.txt', '1170.ann', '1170.txt', '1171.ann', '1171.txt', '1172.ann', '1172.txt', '1173.ann', '1173.txt', '1174.ann', '1174.txt', '1175.ann', '1175.txt', '1176.ann', '1176.txt', '1177.ann', '1177.txt', '1178.ann', '1178.txt', '1179.ann', '1179.txt', '118.ann', '118.txt', '1180.ann', '1180.txt', '1181.ann', '1181.txt', '1182.ann', '1182.txt', '1183.ann', '1183.txt', '1184.ann', '1184.txt', '1185.ann', '1185.txt', '1186.ann', '1186.txt', '1187.ann', '1187.txt', '1188.ann', '1188.txt', '1189.ann', '1189.txt', '119.ann', '119.txt', '1190.ann', '1190.txt', '1191.ann', '1191.txt', '1192.ann', '1192.txt', '1193.ann', '1193.txt', '1194.ann', '1194.txt', '1195.ann', '1195.txt', '1196.ann', '1196.txt', '1197.ann', '1197.txt', '1198.ann', '1198.txt', '1199.ann', '1199.txt', '11_01_13b.ann', '11_01_13b.txt', '11_01_13e.ann', '11_01_13e.txt', '120.ann', '120.txt', '1200.ann', '1200.txt', '121.ann', '121.txt', '122.ann', '122.txt', '123.ann', '123.txt', '124.ann', '124.txt', '125.ann', '125.txt', '126.ann', '126.txt', '127.ann', '127.txt', '128.ann', '128.txt', '129.ann', '129.txt', '130.ann', '130.txt', '131.ann', '131.txt', '132.ann', '132.txt', '133.ann', '133.txt', '134.ann', '134.txt', '135.ann', '135.txt', '136.ann', '136.txt', '137.ann', '137.txt', '138.ann', '138.txt', '139.ann', '139.txt', '140.ann', '140.txt', '141.ann', '141.txt', '142.ann', '142.txt', '143.ann', '143.txt', '144.ann', '144.txt', '145.ann', '145.txt', '146.ann', '146.txt', '147.ann', '147.txt', '148.ann', '148.txt', '149.ann', '149.txt', '14_01_13c.ann', '14_01_13c.txt', '14_01_13g.ann', '14_01_13g.txt', '14_01_13i.ann', '14_01_13i.txt', '150.ann', '150.txt', '151.ann', '151.txt', '152.ann', '152.txt', '153.ann', '153.txt', '154.ann', '154.txt', '155.ann', '155.txt', '156.ann', '156.txt', '157.ann', '157.txt', '158.ann', '158.txt', '159.ann', '159.txt', '15_01_13a.ann', '15_01_13a.txt', '15_01_13b.ann', '15_01_13b.txt', '15_01_13e.ann', '15_01_13e.txt', '15_01_13f.ann', '15_01_13f.txt', '160.ann', '160.txt', '161.ann', '161.txt', '162.ann', '162.txt', '163.ann', '163.txt', '164.ann', '164.txt', '165.ann', '165.txt', '166.ann', '166.txt', '167.ann', '167.txt', '168.ann', '168.txt', '169.ann', '169.txt', '170.ann', '170.txt', '171.ann', '171.txt', '172.ann', '172.txt', '173.ann', '173.txt', '174.ann', '174.txt', '175.ann', '175.txt', '176.ann', '176.txt', '177.ann', '177.txt', '178.ann', '178.txt', '179.ann', '179.txt', '180.ann', '180.txt', '181.ann', '181.txt', '182.ann', '182.txt', '183.ann', '183.txt', '184.ann', '184.txt', '185.ann', '185.txt', '186.ann', '186.txt', '187.ann', '187.txt', '188.ann', '188.txt', '189.ann', '189.txt', '190.ann', '190.txt', '191.ann', '191.txt', '192.ann', '192.txt', '193.ann', '193.txt', '194.ann', '194.txt', '195.ann', '195.txt', '196.ann', '196.txt', '197.ann', '197.txt', '198.ann', '198.txt', '199.ann', '199.txt', '19_11_12d.ann', '19_11_12d.txt', '19_11_12h.ann', '19_11_12h.txt', '200.ann', '200.txt', '2001.ann', '2001.txt', '2002.ann', '2002.txt', '2003.ann', '2003.txt', '2004.ann', '2004.txt', '2005.ann', '2005.txt', '2006.ann', '2006.txt', '2007.ann', '2007.txt', '2008.ann', '2008.txt', '2009.ann', '2009.txt', '201.ann', '201.txt', '2010.ann', '2010.txt', '2011.ann', '2011.txt', '2012.ann', '2012.txt', '2013.ann', '2013.txt', '2014.ann', '2014.txt', '2015.ann', '2015.txt', '2016.ann', '2016.txt', '2017.ann', '2017.txt', '2018.ann', '2018.txt', '2019.ann', '2019.txt', '202.ann', '202.txt', '2020.ann', '2020.txt', '2021.ann', '2021.txt', '2022.ann', '2022.txt', '2023.ann', '2023.txt', '2024.ann', '2024.txt', '2025.ann', '2025.txt', '2026.ann', '2026.txt', '2027.ann', '2027.txt', '2028.ann', '2028.txt', '2029.ann', '2029.txt', '203.ann', '203.txt', '2030.ann', '2030.txt', '2031.ann', '2031.txt', '2032.ann', '2032.txt', '2034.ann', '2034.txt', '2035.ann', '2035.txt', '2036.ann', '2036.txt', '2037.ann', '2037.txt', '2038.ann', '2038.txt', '2039.ann', '2039.txt', '204.ann', '204.txt', '2040.ann', '2040.txt', '2041.ann', '2041.txt', '2042.ann', '2042.txt', '2043.ann', '2043.txt', '2044.ann', '2044.txt', '2045.ann', '2045.txt', '2046.ann', '2046.txt', '2047.ann', '2047.txt', '2048.ann', '2048.txt', '2049.ann', '2049.txt', '205.ann', '205.txt', '2050.ann', '2050.txt', '206.ann', '206.txt', '207.ann', '207.txt', '208.ann', '208.txt', '209.ann', '209.txt', '20_11_12a.ann', '20_11_12a.txt', '20_11_12b.ann', '20_11_12b.txt', '20_11_12c.ann', '20_11_12c.txt', '20_11_12d.ann', '20_11_12d.txt', '20_11_12i.ann', '20_11_12i.txt', '210.ann', '210.txt', '211.ann', '211.txt', '212.ann', '212.txt', '213.ann', '213.txt', '214.ann', '214.txt', '215.ann', '215.txt', '216.ann', '216.txt', '217.ann', '217.txt', '218.ann', '218.txt', '219.ann', '219.txt', '21_11_12c.ann', '21_11_12c.txt', '21_11_12h.ann', '21_11_12h.txt', '21_11_12i.ann', '21_11_12i.txt', '21_11_12j.ann', '21_11_12j.txt', '220.ann', '220.txt', '221.ann', '221.txt', '222.ann', '222.txt', '223.ann', '223.txt', '224.ann', '224.txt', '225.ann', '225.txt', '226.ann', '226.txt', '227.ann', '227.txt', '228.ann', '228.txt', '229.ann', '229.txt', '22_11_12a.ann', '22_11_12a.txt', '22_11_12c.ann', '22_11_12c.txt', '22_11_12d.ann', '22_11_12d.txt', '22_11_12g.ann', '22_11_12g.txt', '22_11_12h.ann', '22_11_12h.txt', '22_11_12i.ann', '22_11_12i.txt', '22_11_12j.ann', '22_11_12j.txt', '230.ann', '230.txt', '231.ann', '231.txt', '232.ann', '232.txt', '233.ann', '233.txt', '234.ann', '234.txt', '235.ann', '235.txt', '236.ann', '236.txt', '237.ann', '237.txt', '238.ann', '238.txt', '239.ann', '239.txt', '23_11_12a.ann', '23_11_12a.txt', '23_11_12b.ann', '23_11_12b.txt', '23_11_12c.ann', '23_11_12c.txt', '23_11_12d.ann', '23_11_12d.txt', '23_11_12e.ann', '23_11_12e.txt', '23_11_12f.ann', '23_11_12f.txt', '240.ann', '240.txt', '241.ann', '241.txt', '242.ann', '242.txt', '243.ann', '243.txt', '244.ann', '244.txt', '245.ann', '245.txt', '246.ann', '246.txt', '247.ann', '247.txt', '248.ann', '248.txt', '249.ann', '249.txt', '250.ann', '250.txt', '251.ann', '251.txt', '252.ann', '252.txt', '253.ann', '253.txt', '254.ann', '254.txt', '255.ann', '255.txt', '256.ann', '256.txt', '257.ann', '257.txt', '258.ann', '258.txt', '259.ann', '259.txt', '25_12_12a.ann', '25_12_12a.txt', '25_12_12c.ann', '25_12_12c.txt', '25_12_12d.ann', '25_12_12d.txt', '25_12_12e.ann', '25_12_12e.txt', '260.ann', '260.txt', '261.ann', '261.txt', '262.ann', '262.txt', '263.ann', '263.txt', '264.ann', '264.txt', '265.ann', '265.txt', '266.ann', '266.txt', '267.ann', '267.txt', '268.ann', '268.txt', '269.ann', '269.txt', '26_11_12b.ann', '26_11_12b.txt', '26_11_12c.ann', '26_11_12c.txt', '26_11_12e.ann', '26_11_12e.txt', '26_11_12f.ann', '26_11_12f.txt', '270.ann', '270.txt', '271.ann', '271.txt', '272.ann', '272.txt', '273.ann', '273.txt', '274.ann', '274.txt', '275.ann', '275.txt', '276.ann', '276.txt', '277.ann', '277.txt', '278.ann', '278.txt', '279.ann', '279.txt', '27_11_12a.ann', '27_11_12a.txt', '27_11_12c.ann', '27_11_12c.txt', '27_11_12d.ann', '27_11_12d.txt', '27_11_12e.ann', '27_11_12e.txt', '27_11_12j.ann', '27_11_12j.txt', '280.ann', '280.txt', '281.ann', '281.txt', '282.ann', '282.txt', '283.ann', '283.txt', '284.ann', '284.txt', '285.ann', '285.txt', '286.ann', '286.txt', '287.ann', '287.txt', '288.ann', '288.txt', '289.ann', '289.txt', '28_11_12a.ann', '28_11_12a.txt', '28_11_12f.ann', '28_11_12f.txt', '28_11_12g.ann', '28_11_12g.txt', '28_11_12h.ann', '28_11_12h.txt', '28_11_12i.ann', '28_11_12i.txt', '28_11_12j.ann', '28_11_12j.txt', '290.ann', '290.txt', '291.ann', '291.txt', '292.ann', '292.txt', '293.ann', '293.txt', '294.ann', '294.txt', '295.ann', '295.txt', '296.ann', '296.txt', '297.ann', '297.txt', '298.ann', '298.txt', '299.ann', '299.txt', '29_11_12a.ann', '29_11_12a.txt', '29_11_12b.ann', '29_11_12b.txt', '300.ann', '300.txt', '301.ann', '301.txt', '302.ann', '302.txt', '303.ann', '303.txt', '304.ann', '304.txt', '305.ann', '305.txt', '306.ann', '306.txt', '307.ann', '307.txt', '308.ann', '308.txt', '309.ann', '309.txt', '30_11_12b.ann', '30_11_12b.txt', '30_11_12h.ann', '30_11_12h.txt', '30_11_12i.ann', '30_11_12i.txt', '310.ann', '310.txt', '311.ann', '311.txt', '312.ann', '312.txt', '313.ann', '313.txt', '314.ann', '314.txt', '315.ann', '315.txt', '316.ann', '316.txt', '317.ann', '317.txt', '318.ann', '318.txt', '319.ann', '319.txt', '320.ann', '320.txt', '321.ann', '321.txt', '322.ann', '322.txt', '323.ann', '323.txt', '324.ann', '324.txt', '325.ann', '325.txt', '326.ann', '326.txt', '327.ann', '327.txt', '328.ann', '328.txt', '329.ann', '329.txt', '330.ann', '330.txt', '331.ann', '331.txt', '332.ann', '332.txt', '333.ann', '333.txt', '334.ann', '334.txt', '335.ann', '335.txt', '336.ann', '336.txt', '337.ann', '337.txt', '338.ann', '338.txt', '339.ann', '339.txt', '340.ann', '340.txt', '341.ann', '341.txt', '342.ann', '342.txt', '343.ann', '343.txt', '344.ann', '344.txt', '345.ann', '345.txt', '346.ann', '346.txt', '347.ann', '347.txt', '348.ann', '348.txt', '349.ann', '349.txt', '350.ann', '350.txt', '351.ann', '351.txt', '352.ann', '352.txt', '353.ann', '353.txt', '354.ann', '354.txt', '355.ann', '355.txt', '356.ann', '356.txt', '357.ann', '357.txt', '358.ann', '358.txt', '359.ann', '359.txt', '360.ann', '360.txt', '361.ann', '361.txt', '362.ann', '362.txt', '363.ann', '363.txt', '364.ann', '364.txt', '365.ann', '365.txt', '366.ann', '366.txt', '367.ann', '367.txt', '368.ann', '368.txt', '369.ann', '369.txt', '370.ann', '370.txt', '371.ann', '371.txt', '372.ann', '372.txt', '373.ann', '373.txt', '374.ann', '374.txt', '375.ann', '375.txt', '376.ann', '376.txt', '377.ann', '377.txt', '378.ann', '378.txt', '379.ann', '379.txt', '380.ann', '380.txt', '381.ann', '381.txt', '382.ann', '382.txt', '383.ann', '383.txt', '384.ann', '384.txt', '385.ann', '385.txt', '386.ann', '386.txt', '387.ann', '387.txt', '388.ann', '388.txt', '389.ann', '389.txt', '390.ann', '390.txt', '391.ann', '391.txt', '392.ann', '392.txt', '393.ann', '393.txt', '394.ann', '394.txt', '395.ann', '395.txt', '396.ann', '396.txt', '397.ann', '397.txt', '398.ann', '398.txt', '399.ann', '399.txt', '400.ann', '400.txt', '401.ann', '401.txt', '402.ann', '402.txt', '403.ann', '403.txt', '404.ann', '404.txt', '405.ann', '405.txt', '406.ann', '406.txt', '407.ann', '407.txt', '408.ann', '408.txt', '409.ann', '409.txt', '410.ann', '410.txt', '411.ann', '411.txt', '412.ann', '412.txt', '413.ann', '413.txt', '414.ann', '414.txt', '415.ann', '415.txt', '416.ann', '416.txt', '417.ann', '417.txt', '418.ann', '418.txt', '419.ann', '419.txt', '420.ann', '420.txt', '421.ann', '421.txt', '422.ann', '422.txt', '423.ann', '423.txt', '424.ann', '424.txt', '425.ann', '425.txt', '426.ann', '426.txt', '427.ann', '427.txt', '428.ann', '428.txt', '429.ann', '429.txt', '430.ann', '430.txt', '431.ann', '431.txt', '432.ann', '432.txt', '433.ann', '433.txt', '434.ann', '434.txt', '435.ann', '435.txt', '436.ann', '436.txt', '437.ann', '437.txt', '438.ann', '438.txt', '439.ann', '439.txt', '440.ann', '440.txt', '441.ann', '441.txt', '442.ann', '442.txt', '443.ann', '443.txt', '444.ann', '444.txt', '445.ann', '445.txt', '446.ann', '446.txt', '447.ann', '447.txt', '448.ann', '448.txt', '449.ann', '449.txt', '450.ann', '450.txt', '451.ann', '451.txt', '452.ann', '452.txt', '453.ann', '453.txt', '454.ann', '454.txt', '455.ann', '455.txt', '457.ann', '457.txt', '458.ann', '458.txt', '459.ann', '459.txt', '460.ann', '460.txt', '461.ann', '461.txt', '462.ann', '462.txt', '463.ann', '463.txt', '464.ann', '464.txt', '465.ann', '465.txt', '466.ann', '466.txt', '467.ann', '467.txt', '468.ann', '468.txt', '469.ann', '469.txt', '470.ann', '470.txt', '471.ann', '471.txt', '472.ann', '472.txt', '473.ann', '473.txt', '474.ann', '474.txt', '475.ann', '475.txt', '476.ann', '476.txt', '477.ann', '477.txt', '478.ann', '478.txt', '479.ann', '479.txt', '480.ann', '480.txt', '481.ann', '481.txt', '482.ann', '482.txt', '483.ann', '483.txt', '484.ann', '484.txt', '485.ann', '485.txt', '486.ann', '486.txt', '487.ann', '487.txt', '488.ann', '488.txt', '489.ann', '489.txt', '490.ann', '490.txt', '491.ann', '491.txt', '492.ann', '492.txt', '493.ann', '493.txt', '494.ann', '494.txt', '495.ann', '495.txt', '496.ann', '496.txt', '497.ann', '497.txt', '498.ann', '498.txt', '499.ann', '499.txt', '500.ann', '500.txt', '501.ann', '501.txt', '502.ann', '502.txt', '503.ann', '503.txt', '504.ann', '504.txt', '505.ann', '505.txt', '506.ann', '506.txt', '507.ann', '507.txt', '508.ann', '508.txt', '509.ann', '509.txt', '510.ann', '510.txt', '511.ann', '511.txt', '512.ann', '512.txt', '513.ann', '513.txt', '514.ann', '514.txt', '515.ann', '515.txt', '516.ann', '516.txt', '517.ann', '517.txt', '518.ann', '518.txt', '519.ann', '519.txt', '520.ann', '520.txt', '521.ann', '521.txt', '522.ann', '522.txt', '523.ann', '523.txt', '524.ann', '524.txt', '525.ann', '525.txt', '526.ann', '526.txt', '527.ann', '527.txt', '528.ann', '528.txt', '529.ann', '529.txt', '530.ann', '530.txt', '531.ann', '531.txt', '532.ann', '532.txt', '533 (!).ann', '533 (!).txt', '534.ann', '534.txt', '535.ann', '535.txt', '536.ann', '536.txt', '537.ann', '537.txt', '538.ann', '538.txt', '539.ann', '539.txt', '540.ann', '540.txt', '541.ann', '541.txt', '542.ann', '542.txt', '543.ann', '543.txt', '544.ann', '544.txt', '545.ann', '545.txt', '546.ann', '546.txt', '547.ann', '547.txt', '548.ann', '548.txt', '549.ann', '549.txt', '550.ann', '550.txt', '551.ann', '551.txt', '552.ann', '552.txt', '553.ann', '553.txt', '554.ann', '554.txt', '555 (!).ann', '555 (!).txt', '556.ann', '556.txt', '557.ann', '557.txt', '558.ann', '558.txt', '559.ann', '559.txt', '560.ann', '560.txt', '561.ann', '561.txt', '562.ann', '562.txt', '563.ann', '563.txt', '564.ann', '564.txt', '565.ann', '565.txt', '567.ann', '567.txt', '568.ann', '568.txt', '569.ann', '569.txt', '570.ann', '570.txt', '571.ann', '571.txt', '572.ann', '572.txt', '574.ann', '574.txt', '575.ann', '575.txt', '576.ann', '576.txt', '577.ann', '577.txt', '578.ann', '578.txt', '579.ann', '579.txt', '581.ann', '581.txt', '582.ann', '582.txt', '583.ann', '583.txt', '584 (!).ann', '584 (!).txt', '585.ann', '585.txt', '586.ann', '586.txt', '587.ann', '587.txt', '588.ann', '588.txt', '589.ann', '589.txt', '590.ann', '590.txt', '591.ann', '591.txt', '592.ann', '592.txt', '593.ann', '593.txt', '594.ann', '594.txt', '596.ann', '596.txt', '597.ann', '597.txt', '598 (!).ann', '598 (!).txt', '599.ann', '599.txt', '600.ann', '600.txt', '601.ann', '601.txt', '602.ann', '602.txt', '610.ann', '610.txt', '611.ann', '611.txt', '612.ann', '612.txt', '613.ann', '613.txt', '614.ann', '614.txt', '615.ann', '615.txt', '616.ann', '616.txt', '617.ann', '617.txt', '618.ann', '618.txt', '619.ann', '619.txt', '620.ann', '620.txt', '621.ann', '621.txt', '622.ann', '622.txt', '623.ann', '623.txt', '624.ann', '624.txt', '625.ann', '625.txt', '626.ann', '626.txt', '627.ann', '627.txt', '628.ann', '628.txt', '629.ann', '629.txt', '630.ann', '630.txt', '631.ann', '631.txt', '632.ann', '632.txt', '633.ann', '633.txt', 'abdulatipov.ann', 'abdulatipov.txt', 'artjakov.ann', 'artjakov.txt', 'Avtovaz.ann', 'Avtovaz.txt', 'blokhin.ann', 'blokhin.txt', 'chaves.ann', 'chaves.txt', 'chirkunov.ann', 'chirkunov.txt', 'kamchatka.ann', 'kamchatka.txt', 'klinton.ann', 'klinton.txt', 'kuleshov.ann', 'kuleshov.txt', 'last_01.ann', 'last_01.txt', 'last_02.ann', 'last_02.txt', 'last_03.ann', 'last_03.txt', 'last_04.ann', 'last_04.txt', 'last_05.ann', 'last_05.txt', 'last_06.ann', 'last_06.txt', 'last_07_new.ann', 'last_07_new.txt', 'last_08.ann', 'last_08.txt', 'last_09.ann', 'last_09.txt', 'last_10.ann', 'last_10.txt', 'last_11.ann', 'last_11.txt', 'last_12.ann', 'last_12.txt', 'last_13.ann', 'last_13.txt', 'last_14.ann', 'last_14.txt', 'last_15.ann', 'last_15.txt', 'last_16.ann', 'last_16.txt', 'last_17.ann', 'last_17.txt', 'last_18.ann', 'last_18.txt', 'last_19.ann', 'last_19.txt', 'last_20.ann', 'last_20.txt', 'last_21.ann', 'last_21.txt', 'last_22.ann', 'last_22.txt', 'last_23.ann', 'last_23.txt', 'last_24.ann', 'last_24.txt', 'last_25.ann', 'last_25.txt', 'last_26.ann', 'last_26.txt', 'last_27.ann', 'last_27.txt', 'last_28.ann', 'last_28.txt', 'last_29.ann', 'last_29.txt', 'last_30_new.ann', 'last_30_new.txt', 'last_31.ann', 'last_31.txt', 'last_32.ann', 'last_32.txt', 'last_33.ann', 'last_33.txt', 'last_34.ann', 'last_34.txt', 'last_35.ann', 'last_35.txt', 'last_36.ann', 'last_36.txt', 'last_37.ann', 'last_37.txt', 'last_38.ann', 'last_38.txt', 'last_39.ann', 'last_39.txt', 'last_40.ann', 'last_40.txt', 'last_41.ann', 'last_41.txt', 'last_42.ann', 'last_42.txt', 'last_43.ann', 'last_43.txt', 'last_44.ann', 'last_44.txt', 'last_45.ann', 'last_45.txt', 'last_46.ann', 'last_46.txt', 'last_47.ann', 'last_47.txt', 'last_48.ann', 'last_48.txt', 'last_49.ann', 'last_49.txt', 'last_50.ann', 'last_50.txt', 'last_51.ann', 'last_51.txt', 'last_52.ann', 'last_52.txt', 'last_53.ann', 'last_53.txt', 'last_54.ann', 'last_54.txt', 'last_55.ann', 'last_55.txt', 'last_56.ann', 'last_56.txt', 'last_57.ann', 'last_57.txt', 'last_58.ann', 'last_58.txt', 'last_59.ann', 'last_59.txt', 'last_60.ann', 'last_60.txt', 'last_61.ann', 'last_61.txt', 'last_62.ann', 'last_62.txt', 'last_63.ann', 'last_63.txt', 'last_64.ann', 'last_64.txt', 'last_65.ann', 'last_65.txt', 'last_66.ann', 'last_66.txt', 'last_67.ann', 'last_67.txt', 'last_68.ann', 'last_68.txt', 'last_69.ann', 'last_69.txt', 'last_70.ann', 'last_70.txt', 'last_71.ann', 'last_71.txt', 'last_72.ann', 'last_72.txt', 'last_73.ann', 'last_73.txt', 'last_74.ann', 'last_74.txt', 'last_75.ann', 'last_75.txt', 'lenoblast.ann', 'lenoblast.txt', 'maykl dzhekson.ann', 'maykl dzhekson.txt', 'mvd.ann', 'mvd.txt', 'mvd2.ann', 'mvd2.txt', 'rosobrnadzor.ann', 'rosobrnadzor.txt', 'ryadovoy chelah.ann', 'ryadovoy chelah.txt', 'semenenko.ann', 'semenenko.txt', 'shojgu1.ann', 'shojgu1.txt', 'shojgu3.ann', 'shojgu3.txt', 'shojgu4.ann', 'shojgu4.txt', 'shojgu6.ann', 'shojgu6.txt', 'si_tzjanpin.ann', 'si_tzjanpin.txt', 'sobjanin2.ann', 'sobjanin2.txt', 'turkmenija.ann', 'turkmenija.txt', 'uchitel.ann', 'uchitel.txt']\n"
     ]
    }
   ],
   "source": [
    "#просматриваем содержание коллекции\n",
    "print(os.listdir(\"collection3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.txt', '002.txt', '003.txt', '004.txt', '005.txt', '006.txt', '007.txt', '008.txt', '009.txt', '010.txt', '011.txt', '012.txt', '013.txt', '014.txt', '015 (!).txt', '016.txt', '017.txt', '018.txt', '019.txt', '020.txt', '021.txt', '022.txt', '023.txt', '025.txt', '026.txt', '027.txt', '028.txt', '029.txt', '030.txt', '031.txt', '032.txt', '033.txt', '034.txt', '035.txt', '036.txt', '037.txt', '038.txt', '039.txt', '03_12_12a.txt', '03_12_12b.txt', '03_12_12c.txt', '03_12_12d.txt', '03_12_12g.txt', '03_12_12h.txt', '040.txt', '041.txt', '042.txt', '043.txt', '044.txt', '045.txt', '046.txt', '047.txt', '048.txt', '049.txt', '04_02_13a_abdulatipov.txt', '04_03_13a_sorokin.txt', '04_12_12b.txt', '04_12_12d.txt', '04_12_12f.txt', '04_12_12g.txt', '04_12_12h_corr.txt', '050.txt', '051.txt', '052.txt', '053.txt', '054.txt', '055.txt', '056.txt', '057.txt', '058.txt', '059.txt', '060.txt', '061.txt', '062.txt', '063.txt', '064.txt', '065.txt', '066.txt', '067.txt', '068.txt', '069.txt', '070.txt', '071.txt', '072.txt', '073.txt', '074.txt', '075.txt', '076.txt', '077.txt', '078.txt', '079.txt', '080.txt', '081.txt', '082.txt', '083.txt', '084.txt', '085.txt', '086.txt', '087.txt', '088.txt', '089.txt', '090.txt', '091.txt', '092.txt', '093.txt', '094.txt', '095.txt', '096.txt', '097.txt', '098.txt', '099.txt', '09_01_13.txt', '09_01_13a.txt', '09_01_13c.txt', '09_01_13d.txt', '09_01_13e.txt', '09_01_13h.txt', '09_01_13i.txt', '100.txt', '1000.txt', '1001.txt', '1002.txt', '1003.txt', '1004.txt', '1005.txt', '1006.txt', '1007.txt', '1008.txt', '1009.txt', '101.txt', '1010.txt', '1011.txt', '1012.txt', '1013.txt', '1014.txt', '1015.txt', '1016.txt', '1017.txt', '1018.txt', '1019.txt', '102.txt', '1020.txt', '1021.txt', '1022.txt', '1023.txt', '1024.txt', '1025.txt', '1026.txt', '1027.txt', '1028.txt', '1029.txt', '103.txt', '1030.txt', '1031.txt', '1032.txt', '1033.txt', '1034.txt', '1035.txt', '1036.txt', '1037.txt', '1038.txt', '1039.txt', '104.txt', '1040.txt', '1041.txt', '1042.txt', '1043.txt', '1044.txt', '1045.txt', '1046.txt', '1047.txt', '1048.txt', '1049.txt', '105.txt', '1050.txt', '106.txt', '107.txt', '108.txt', '109.txt', '10_01_13a.txt', '10_01_13d.txt', '10_01_13i.txt', '110.txt', '1100.txt', '1101.txt', '1102.txt', '1103.txt', '1104.txt', '1105.txt', '1106.txt', '1107.txt', '1108.txt', '1109.txt', '111.txt', '1110.txt', '1111.txt', '1112.txt', '1113.txt', '1114.txt', '1115.txt', '1116.txt', '1117.txt', '1118.txt', '1119.txt', '112.txt', '1120.txt', '1121.txt', '1122.txt', '1123.txt', '1124.txt', '1125.txt', '1126.txt', '1127.txt', '1128.txt', '113.txt', '1130.txt', '1131.txt', '1132.txt', '1133.txt', '1134.txt', '1135.txt', '1136.txt', '1137.txt', '1138.txt', '1139.txt', '114.txt', '1140.txt', '1141.txt', '1142.txt', '1143.txt', '1144.txt', '1145.txt', '1146.txt', '1147.txt', '1148.txt', '1149.txt', '115.txt', '1150.txt', '1151.txt', '1152.txt', '1153.txt', '1154.txt', '1155.txt', '1156.txt', '1157.txt', '1158.txt', '1159.txt', '116.txt', '1160.txt', '1161.txt', '1162.txt', '1163.txt', '1164.txt', '1165.txt', '1166.txt', '1167.txt', '1168.txt', '1169.txt', '117.txt', '1170.txt', '1171.txt', '1172.txt', '1173.txt', '1174.txt', '1175.txt', '1176.txt', '1177.txt', '1178.txt', '1179.txt', '118.txt', '1180.txt', '1181.txt', '1182.txt', '1183.txt', '1184.txt', '1185.txt', '1186.txt', '1187.txt', '1188.txt', '1189.txt', '119.txt', '1190.txt', '1191.txt', '1192.txt', '1193.txt', '1194.txt', '1195.txt', '1196.txt', '1197.txt', '1198.txt', '1199.txt', '11_01_13b.txt', '11_01_13e.txt', '120.txt', '1200.txt', '121.txt', '122.txt', '123.txt', '124.txt', '125.txt', '126.txt', '127.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '133.txt', '134.txt', '135.txt', '136.txt', '137.txt', '138.txt', '139.txt', '140.txt', '141.txt', '142.txt', '143.txt', '144.txt', '145.txt', '146.txt', '147.txt', '148.txt', '149.txt', '14_01_13c.txt', '14_01_13g.txt', '14_01_13i.txt', '150.txt', '151.txt', '152.txt', '153.txt', '154.txt', '155.txt', '156.txt', '157.txt', '158.txt', '159.txt', '15_01_13a.txt', '15_01_13b.txt', '15_01_13e.txt', '15_01_13f.txt', '160.txt', '161.txt', '162.txt', '163.txt', '164.txt', '165.txt', '166.txt', '167.txt', '168.txt', '169.txt', '170.txt', '171.txt', '172.txt', '173.txt', '174.txt', '175.txt', '176.txt', '177.txt', '178.txt', '179.txt', '180.txt', '181.txt', '182.txt', '183.txt', '184.txt', '185.txt', '186.txt', '187.txt', '188.txt', '189.txt', '190.txt', '191.txt', '192.txt', '193.txt', '194.txt', '195.txt', '196.txt', '197.txt', '198.txt', '199.txt', '19_11_12d.txt', '19_11_12h.txt', '200.txt', '2001.txt', '2002.txt', '2003.txt', '2004.txt', '2005.txt', '2006.txt', '2007.txt', '2008.txt', '2009.txt', '201.txt', '2010.txt', '2011.txt', '2012.txt', '2013.txt', '2014.txt', '2015.txt', '2016.txt', '2017.txt', '2018.txt', '2019.txt', '202.txt', '2020.txt', '2021.txt', '2022.txt', '2023.txt', '2024.txt', '2025.txt', '2026.txt', '2027.txt', '2028.txt', '2029.txt', '203.txt', '2030.txt', '2031.txt', '2032.txt', '2034.txt', '2035.txt', '2036.txt', '2037.txt', '2038.txt', '2039.txt', '204.txt', '2040.txt', '2041.txt', '2042.txt', '2043.txt', '2044.txt', '2045.txt', '2046.txt', '2047.txt', '2048.txt', '2049.txt', '205.txt', '2050.txt', '206.txt', '207.txt', '208.txt', '209.txt', '20_11_12a.txt', '20_11_12b.txt', '20_11_12c.txt', '20_11_12d.txt', '20_11_12i.txt', '210.txt', '211.txt', '212.txt', '213.txt', '214.txt', '215.txt', '216.txt', '217.txt', '218.txt', '219.txt', '21_11_12c.txt', '21_11_12h.txt', '21_11_12i.txt', '21_11_12j.txt', '220.txt', '221.txt', '222.txt', '223.txt', '224.txt', '225.txt', '226.txt', '227.txt', '228.txt', '229.txt', '22_11_12a.txt', '22_11_12c.txt', '22_11_12d.txt', '22_11_12g.txt', '22_11_12h.txt', '22_11_12i.txt', '22_11_12j.txt', '230.txt', '231.txt', '232.txt', '233.txt', '234.txt', '235.txt', '236.txt', '237.txt', '238.txt', '239.txt', '23_11_12a.txt', '23_11_12b.txt', '23_11_12c.txt', '23_11_12d.txt', '23_11_12e.txt', '23_11_12f.txt', '240.txt', '241.txt', '242.txt', '243.txt', '244.txt', '245.txt', '246.txt', '247.txt', '248.txt', '249.txt', '250.txt', '251.txt', '252.txt', '253.txt', '254.txt', '255.txt', '256.txt', '257.txt', '258.txt', '259.txt', '25_12_12a.txt', '25_12_12c.txt', '25_12_12d.txt', '25_12_12e.txt', '260.txt', '261.txt', '262.txt', '263.txt', '264.txt', '265.txt', '266.txt', '267.txt', '268.txt', '269.txt', '26_11_12b.txt', '26_11_12c.txt', '26_11_12e.txt', '26_11_12f.txt', '270.txt', '271.txt', '272.txt', '273.txt', '274.txt', '275.txt', '276.txt', '277.txt', '278.txt', '279.txt', '27_11_12a.txt', '27_11_12c.txt', '27_11_12d.txt', '27_11_12e.txt', '27_11_12j.txt', '280.txt', '281.txt', '282.txt', '283.txt', '284.txt', '285.txt', '286.txt', '287.txt', '288.txt', '289.txt', '28_11_12a.txt', '28_11_12f.txt', '28_11_12g.txt', '28_11_12h.txt', '28_11_12i.txt', '28_11_12j.txt', '290.txt', '291.txt', '292.txt', '293.txt', '294.txt', '295.txt', '296.txt', '297.txt', '298.txt', '299.txt', '29_11_12a.txt', '29_11_12b.txt', '300.txt', '301.txt', '302.txt', '303.txt', '304.txt', '305.txt', '306.txt', '307.txt', '308.txt', '309.txt', '30_11_12b.txt', '30_11_12h.txt', '30_11_12i.txt', '310.txt', '311.txt', '312.txt', '313.txt', '314.txt', '315.txt', '316.txt', '317.txt', '318.txt', '319.txt', '320.txt', '321.txt', '322.txt', '323.txt', '324.txt', '325.txt', '326.txt', '327.txt', '328.txt', '329.txt', '330.txt', '331.txt', '332.txt', '333.txt', '334.txt', '335.txt', '336.txt', '337.txt', '338.txt', '339.txt', '340.txt', '341.txt', '342.txt', '343.txt', '344.txt', '345.txt', '346.txt', '347.txt', '348.txt', '349.txt', '350.txt', '351.txt', '352.txt', '353.txt', '354.txt', '355.txt', '356.txt', '357.txt', '358.txt', '359.txt', '360.txt', '361.txt', '362.txt', '363.txt', '364.txt', '365.txt', '366.txt', '367.txt', '368.txt', '369.txt', '370.txt', '371.txt', '372.txt', '373.txt', '374.txt', '375.txt', '376.txt', '377.txt', '378.txt', '379.txt', '380.txt', '381.txt', '382.txt', '383.txt', '384.txt', '385.txt', '386.txt', '387.txt', '388.txt', '389.txt', '390.txt', '391.txt', '392.txt', '393.txt', '394.txt', '395.txt', '396.txt', '397.txt', '398.txt', '399.txt', '400.txt', '401.txt', '402.txt', '403.txt', '404.txt', '405.txt', '406.txt', '407.txt', '408.txt', '409.txt', '410.txt', '411.txt', '412.txt', '413.txt', '414.txt', '415.txt', '416.txt', '417.txt', '418.txt', '419.txt', '420.txt', '421.txt', '422.txt', '423.txt', '424.txt', '425.txt', '426.txt', '427.txt', '428.txt', '429.txt', '430.txt', '431.txt', '432.txt', '433.txt', '434.txt', '435.txt', '436.txt', '437.txt', '438.txt', '439.txt', '440.txt', '441.txt', '442.txt', '443.txt', '444.txt', '445.txt', '446.txt', '447.txt', '448.txt', '449.txt', '450.txt', '451.txt', '452.txt', '453.txt', '454.txt', '455.txt', '457.txt', '458.txt', '459.txt', '460.txt', '461.txt', '462.txt', '463.txt', '464.txt', '465.txt', '466.txt', '467.txt', '468.txt', '469.txt', '470.txt', '471.txt', '472.txt', '473.txt', '474.txt', '475.txt', '476.txt', '477.txt', '478.txt', '479.txt', '480.txt', '481.txt', '482.txt', '483.txt', '484.txt', '485.txt', '486.txt', '487.txt', '488.txt', '489.txt', '490.txt', '491.txt', '492.txt', '493.txt', '494.txt', '495.txt', '496.txt', '497.txt', '498.txt', '499.txt', '500.txt', '501.txt', '502.txt', '503.txt', '504.txt', '505.txt', '506.txt', '507.txt', '508.txt', '509.txt', '510.txt', '511.txt', '512.txt', '513.txt', '514.txt', '515.txt', '516.txt', '517.txt', '518.txt', '519.txt', '520.txt', '521.txt', '522.txt', '523.txt', '524.txt', '525.txt', '526.txt', '527.txt', '528.txt', '529.txt', '530.txt', '531.txt', '532.txt', '533 (!).txt', '534.txt', '535.txt', '536.txt', '537.txt', '538.txt', '539.txt', '540.txt', '541.txt', '542.txt', '543.txt', '544.txt', '545.txt', '546.txt', '547.txt', '548.txt', '549.txt', '550.txt', '551.txt', '552.txt', '553.txt', '554.txt', '555 (!).txt', '556.txt', '557.txt', '558.txt', '559.txt', '560.txt', '561.txt', '562.txt', '563.txt', '564.txt', '565.txt', '567.txt', '568.txt', '569.txt', '570.txt', '571.txt', '572.txt', '574.txt', '575.txt', '576.txt', '577.txt', '578.txt', '579.txt', '581.txt', '582.txt', '583.txt', '584 (!).txt', '585.txt', '586.txt', '587.txt', '588.txt', '589.txt', '590.txt', '591.txt', '592.txt', '593.txt', '594.txt', '596.txt', '597.txt', '598 (!).txt', '599.txt', '600.txt', '601.txt', '602.txt', '610.txt', '611.txt', '612.txt', '613.txt', '614.txt', '615.txt', '616.txt', '617.txt', '618.txt', '619.txt', '620.txt', '621.txt', '622.txt', '623.txt', '624.txt', '625.txt', '626.txt', '627.txt', '628.txt', '629.txt', '630.txt', '631.txt', '632.txt', '633.txt', 'abdulatipov.txt', 'artjakov.txt', 'Avtovaz.txt', 'blokhin.txt', 'chaves.txt', 'chirkunov.txt', 'kamchatka.txt', 'klinton.txt', 'kuleshov.txt', 'last_01.txt', 'last_02.txt', 'last_03.txt', 'last_04.txt', 'last_05.txt', 'last_06.txt', 'last_07_new.txt', 'last_08.txt', 'last_09.txt', 'last_10.txt', 'last_11.txt', 'last_12.txt', 'last_13.txt', 'last_14.txt', 'last_15.txt', 'last_16.txt', 'last_17.txt', 'last_18.txt', 'last_19.txt', 'last_20.txt', 'last_21.txt', 'last_22.txt', 'last_23.txt', 'last_24.txt', 'last_25.txt', 'last_26.txt', 'last_27.txt', 'last_28.txt', 'last_29.txt', 'last_30_new.txt', 'last_31.txt', 'last_32.txt', 'last_33.txt', 'last_34.txt', 'last_35.txt', 'last_36.txt', 'last_37.txt', 'last_38.txt', 'last_39.txt', 'last_40.txt', 'last_41.txt', 'last_42.txt', 'last_43.txt', 'last_44.txt', 'last_45.txt', 'last_46.txt', 'last_47.txt', 'last_48.txt', 'last_49.txt', 'last_50.txt', 'last_51.txt', 'last_52.txt', 'last_53.txt', 'last_54.txt', 'last_55.txt', 'last_56.txt', 'last_57.txt', 'last_58.txt', 'last_59.txt', 'last_60.txt', 'last_61.txt', 'last_62.txt', 'last_63.txt', 'last_64.txt', 'last_65.txt', 'last_66.txt', 'last_67.txt', 'last_68.txt', 'last_69.txt', 'last_70.txt', 'last_71.txt', 'last_72.txt', 'last_73.txt', 'last_74.txt', 'last_75.txt', 'lenoblast.txt', 'maykl dzhekson.txt', 'mvd.txt', 'mvd2.txt', 'rosobrnadzor.txt', 'ryadovoy chelah.txt', 'semenenko.txt', 'shojgu1.txt', 'shojgu3.txt', 'shojgu4.txt', 'shojgu6.txt', 'si_tzjanpin.txt', 'sobjanin2.txt', 'turkmenija.txt', 'uchitel.txt']\n"
     ]
    }
   ],
   "source": [
    "#собираем только текстовые файлы коллекции\n",
    "fileDir = r\"collection3\"\n",
    "fileExt = r\".txt\"\n",
    "documents_txt = [_ for _ in os.listdir(fileDir) if _.endswith(fileExt)]\n",
    "print(documents_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Россия рассчитывает на конструктивное воздейст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Комиссар СЕ критикует ограничительную политику...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Пулеметы, автоматы и снайперские винтовки изъя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 октября назначены очередные выборы Верховног...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Следственное управление при прокуратуре требуе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Депутат от \"ЕР\": К отставке А.Сердюкова причас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\nСи Цзиньпин избран генсеком Коммунистической...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Ведомости\" узнали о смене лидера московских е...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>СМИ узнали о кутежах туркменского чиновника на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Вице-мэром Новосибирска по социальным вопросам...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Россия рассчитывает на конструктивное воздейст...\n",
       "1    Комиссар СЕ критикует ограничительную политику...\n",
       "2    Пулеметы, автоматы и снайперские винтовки изъя...\n",
       "3    4 октября назначены очередные выборы Верховног...\n",
       "4    Следственное управление при прокуратуре требуе...\n",
       "..                                                 ...\n",
       "994  Депутат от \"ЕР\": К отставке А.Сердюкова причас...\n",
       "995  \\nСи Цзиньпин избран генсеком Коммунистической...\n",
       "996  \"Ведомости\" узнали о смене лидера московских е...\n",
       "997  СМИ узнали о кутежах туркменского чиновника на...\n",
       "998  Вице-мэром Новосибирска по социальным вопросам...\n",
       "\n",
       "[999 rows x 1 columns]"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#заносим данные файлов txt в датасет\n",
    "text_list = []\n",
    "for file in documents_txt:\n",
    "    doc = open('collection3/' + file, encoding='utf-8')\n",
    "    text = doc.read()\n",
    "    text_list.append(text)\n",
    "    \n",
    "data_text = pd.DataFrame({'text': text_list })\n",
    "data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. NER из NLTK <a id='section_2.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BazhanovaEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BazhanovaEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#требуется для токенизации\n",
    "nltk.download('punkt')\n",
    "#требуется для parts of speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Россия', 'JJ'),\n",
       " ('рассчитывает', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('конструктивное', 'NNP'),\n",
       " ('воздействие', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Грузию', 'VBD'),\n",
       " ('04/08/2008', 'CD'),\n",
       " ('12:08', 'CD'),\n",
       " ('МОСКВА', 'NN'),\n",
       " (',', ','),\n",
       " ('4', 'CD'),\n",
       " ('авг', 'SYM'),\n",
       " ('-', ':'),\n",
       " ('РИА', 'NN'),\n",
       " ('Новости', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Россия', 'JJ'),\n",
       " ('рассчитывает', 'NN'),\n",
       " (',', ','),\n",
       " ('что', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('воздействуют', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Тбилиси', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('связи', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('обострением', 'NNP'),\n",
       " ('ситуации', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('зоне', 'NNP'),\n",
       " ('грузино-осетинского', 'JJ'),\n",
       " ('конфликта', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Об', 'VB'),\n",
       " ('этом', 'JJ'),\n",
       " ('статс-секретарь', 'JJ'),\n",
       " ('-', ':'),\n",
       " ('заместитель', 'NN'),\n",
       " ('министра', 'JJ'),\n",
       " ('иностранных', 'NNP'),\n",
       " ('дел', 'NNP'),\n",
       " ('России', 'NNP'),\n",
       " ('Григорий', 'NNP'),\n",
       " ('Карасин', 'NNP'),\n",
       " ('заявил', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('телефонном', 'NNP'),\n",
       " ('разговоре', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('заместителем', 'NNP'),\n",
       " ('госсекретаря', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('Дэниэлом', 'NNP'),\n",
       " ('Фридом', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('С', 'JJ'),\n",
       " ('российской', 'NN'),\n",
       " ('стороны', 'NNP'),\n",
       " ('выражена', 'NNP'),\n",
       " ('глубокая', 'NNP'),\n",
       " ('озабоченность', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('связи', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('новым', 'NNP'),\n",
       " ('витком', 'NNP'),\n",
       " ('напряженности', 'NNP'),\n",
       " ('вокруг', 'NNP'),\n",
       " ('Южной', 'NNP'),\n",
       " ('Осетии', 'NNP'),\n",
       " (',', ','),\n",
       " ('противозаконными', 'NNP'),\n",
       " ('действиями', 'NNP'),\n",
       " ('грузинской', 'NNP'),\n",
       " ('стороны', 'NNP'),\n",
       " ('по', 'NNP'),\n",
       " ('наращиванию', 'NNP'),\n",
       " ('своих', 'NNP'),\n",
       " ('вооруженных', 'NNP'),\n",
       " ('сил', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('регионе', 'NNP'),\n",
       " (',', ','),\n",
       " ('бесконтрольным', 'NNP'),\n",
       " ('строительством', 'NNP'),\n",
       " ('фортификационных', 'NNP'),\n",
       " ('сооружений', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('-', ':'),\n",
       " ('говорится', 'NN'),\n",
       " ('в', 'JJ'),\n",
       " ('сообщении', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('Россия', 'JJ'),\n",
       " ('уже', 'NN'),\n",
       " ('призвала', 'NNP'),\n",
       " ('Тбилиси', 'NNP'),\n",
       " ('к', 'NNP'),\n",
       " ('ответственной', 'NNP'),\n",
       " ('линии', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('рассчитывает', 'NNP'),\n",
       " ('также', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('конструктивное', 'NNP'),\n",
       " ('воздействие', 'NNP'),\n",
       " ('со', 'NNP'),\n",
       " ('стороны', 'NNP'),\n",
       " ('Вашингтона', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('-', ':'),\n",
       " ('сообщил', 'NN'),\n",
       " ('МИД', 'JJ'),\n",
       " ('России', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#пример текста\n",
    "document = data_text.text[0]\n",
    "\n",
    "#разбиваем документ на токены и применяем pos tagging (на выходе список кортежей (токен, часть речи))\n",
    "nltk.pos_tag(nltk.word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('МИД России', 'ORGANIZATION'),\n",
       " ('МОСКВА', 'ORGANIZATION'),\n",
       " ('РИА Новости', 'ORGANIZATION'),\n",
       " ('России Григорий Карасин', 'PERSON'),\n",
       " ('Россия', 'PERSON'),\n",
       " ('Тбилиси', 'PERSON')}"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#распознаем именнованные сущности с помощью классификатора (Person, Organization, GPE)\n",
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>LOC 0 6</th>\n",
       "      <th>Россия</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2</td>\n",
       "      <td>LOC 50 53</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>LOC 57 63</td>\n",
       "      <td>Грузию</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T4</td>\n",
       "      <td>LOC 87 93</td>\n",
       "      <td>МОСКВА</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5</td>\n",
       "      <td>ORG 103 114</td>\n",
       "      <td>РИА Новости</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T6</td>\n",
       "      <td>LOC 116 122</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T7</td>\n",
       "      <td>LOC 141 144</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T8</td>\n",
       "      <td>LOC 161 168</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T9</td>\n",
       "      <td>LOC 301 307</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T10</td>\n",
       "      <td>PER 308 324</td>\n",
       "      <td>Григорий Карасин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T11</td>\n",
       "      <td>LOC 383 386</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T12</td>\n",
       "      <td>PER 387 402</td>\n",
       "      <td>Дэниэлом Фридом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T13</td>\n",
       "      <td>LOC 505 517</td>\n",
       "      <td>Южной Осетии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T14</td>\n",
       "      <td>LOC 703 709</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T15</td>\n",
       "      <td>LOC 723 730</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T16</td>\n",
       "      <td>LOC 815 825</td>\n",
       "      <td>Вашингтона</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T17</td>\n",
       "      <td>ORG 838 841</td>\n",
       "      <td>МИД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T18</td>\n",
       "      <td>LOC 842 848</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     T1      LOC 0 6            Россия\n",
       "0    T2    LOC 50 53               США\n",
       "1    T3    LOC 57 63            Грузию\n",
       "2    T4    LOC 87 93            МОСКВА\n",
       "3    T5  ORG 103 114       РИА Новости\n",
       "4    T6  LOC 116 122            Россия\n",
       "5    T7  LOC 141 144               США\n",
       "6    T8  LOC 161 168           Тбилиси\n",
       "7    T9  LOC 301 307            России\n",
       "8   T10  PER 308 324  Григорий Карасин\n",
       "9   T11  LOC 383 386               США\n",
       "10  T12  PER 387 402   Дэниэлом Фридом\n",
       "11  T13  LOC 505 517      Южной Осетии\n",
       "12  T14  LOC 703 709            Россия\n",
       "13  T15  LOC 723 730           Тбилиси\n",
       "14  T16  LOC 815 825        Вашингтона\n",
       "15  T17  ORG 838 841               МИД\n",
       "16  T18  LOC 842 848            России"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разметка из collection3\n",
    "pd.read_csv('collection3/001.ann', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** многие примеры сущностей NER из nltk не обнаружила."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. NER из Spacy <a id='section_2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Россия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " рассчитывает на конструктивное воздействие \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " на \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Грузию\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</br></br>04/08/2008 12:08</br></br>МОСКВА, 4 авг - \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    РИА Новости\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Россия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " рассчитывает, что \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " воздействуют на \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Тбилиси\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    России\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Григорий Карасин\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " заявил в телефонном разговоре с заместителем госсекретаря \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    США\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Дэниэлом Фридом\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ".</br></br>&quot;С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Южной Осетии\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений&quot;, - говорится в сообщении.</br></br>&quot;\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Россия\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " уже призвала \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Тбилиси\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " к ответственной линии и рассчитывает также на конструктивное воздействие со стороны \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Вашингтона\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "&quot;, - сообщил \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    МИД\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    России\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = ru_core_news_sm.load()\n",
    "ny_bb = data_text.text[0]\n",
    "article = nlp(ny_bb)\n",
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Россия PROPN nsubj\n",
      "рассчитывает VERB ROOT\n",
      "на ADP case\n",
      "конструктивное ADJ amod\n",
      "воздействие NOUN obl\n",
      "США PROPN nmod\n",
      "на ADP case\n",
      "Грузию PROPN nmod\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "04/08/2008 NUM flat:foreign\n",
      "12:08 NUM appos\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "МОСКВА PROPN obj\n",
      ", PUNCT punct\n",
      "4 NUM nummod:gov\n",
      "авг NOUN flat\n",
      "- NOUN conj\n",
      "РИА PROPN conj\n",
      "Новости PROPN appos\n",
      ". PUNCT punct\n",
      "Россия PROPN nsubj\n",
      "рассчитывает VERB ROOT\n",
      ", PUNCT punct\n",
      "что SCONJ mark\n",
      "США PROPN nsubj\n",
      "воздействуют VERB ccomp\n",
      "на ADP case\n",
      "Тбилиси PROPN obl\n",
      "в ADP case\n",
      "связи NOUN fixed\n",
      "с ADP fixed\n",
      "обострением NOUN obl\n",
      "ситуации NOUN nmod\n",
      "в ADP case\n",
      "зоне NOUN nmod\n",
      "грузино ADJ amod\n",
      "- ADJ amod\n",
      "осетинского ADJ amod\n",
      "конфликта NOUN nmod\n",
      ". PUNCT punct\n",
      "Об ADP case\n",
      "этом PRON obl\n",
      "статс NOUN nsubj\n",
      "- NOUN nsubj\n",
      "секретарь NOUN nsubj\n",
      "- NOUN nsubj\n",
      "заместитель NOUN nsubj\n",
      "министра NOUN nmod\n",
      "иностранных ADJ amod\n",
      "дел NOUN nmod\n",
      "России PROPN nmod\n",
      "Григорий PROPN appos\n",
      "Карасин PROPN flat:name\n",
      "заявил VERB ROOT\n",
      "в ADP case\n",
      "телефонном ADJ amod\n",
      "разговоре NOUN obl\n",
      "с ADP case\n",
      "заместителем NOUN nmod\n",
      "госсекретаря NOUN nmod\n",
      "США PROPN nmod\n",
      "Дэниэлом PROPN appos\n",
      "Фридом PROPN flat:name\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "\" PUNCT punct\n",
      "С ADP case\n",
      "российской ADJ amod\n",
      "стороны NOUN obl\n",
      "выражена VERB ROOT\n",
      "глубокая ADJ amod\n",
      "озабоченность NOUN nsubj:pass\n",
      "в ADP case\n",
      "связи NOUN fixed\n",
      "с ADP fixed\n",
      "новым ADJ amod\n",
      "витком NOUN obl\n",
      "напряженности NOUN nmod\n",
      "вокруг ADP case\n",
      "Южной ADJ amod\n",
      "Осетии PROPN nmod\n",
      ", PUNCT punct\n",
      "противозаконными ADJ amod\n",
      "действиями NOUN conj\n",
      "грузинской ADJ amod\n",
      "стороны NOUN nmod\n",
      "по ADP case\n",
      "наращиванию NOUN nmod\n",
      "своих DET det\n",
      "вооруженных VERB amod\n",
      "сил NOUN nmod\n",
      "в ADP case\n",
      "регионе NOUN nmod\n",
      ", PUNCT punct\n",
      "бесконтрольным ADJ amod\n",
      "строительством NOUN conj\n",
      "фортификационных ADJ amod\n",
      "сооружений NOUN nmod\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "- PUNCT punct\n",
      "говорится VERB parataxis\n",
      "в ADP case\n",
      "сообщении NOUN obl\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "\" PUNCT punct\n",
      "Россия PROPN nsubj\n",
      "уже ADV advmod\n",
      "призвала VERB ROOT\n",
      "Тбилиси PROPN obj\n",
      "к ADP case\n",
      "ответственной ADJ amod\n",
      "линии NOUN obl\n",
      "и CCONJ cc\n",
      "рассчитывает VERB conj\n",
      "также ADV advmod\n",
      "на ADP case\n",
      "конструктивное ADJ amod\n",
      "воздействие NOUN obl\n",
      "со ADP case\n",
      "стороны NOUN fixed\n",
      "Вашингтона PROPN nmod\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "- PUNCT punct\n",
      "сообщил VERB parataxis\n",
      "МИД PROPN iobj\n",
      "России PROPN nmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "#список токенов, частей речи и сущностей\n",
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>LOC 0 6</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>LOC 50 53</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>LOC 57 63</td>\n",
       "      <td>Грузию</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>LOC 87 93</td>\n",
       "      <td>МОСКВА</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>ORG 103 114</td>\n",
       "      <td>РИА Новости</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T6</td>\n",
       "      <td>LOC 116 122</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T7</td>\n",
       "      <td>LOC 141 144</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T8</td>\n",
       "      <td>LOC 161 168</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T9</td>\n",
       "      <td>LOC 301 307</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T10</td>\n",
       "      <td>PER 308 324</td>\n",
       "      <td>Григорий Карасин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T11</td>\n",
       "      <td>LOC 383 386</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T12</td>\n",
       "      <td>PER 387 402</td>\n",
       "      <td>Дэниэлом Фридом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T13</td>\n",
       "      <td>LOC 505 517</td>\n",
       "      <td>Южной Осетии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T14</td>\n",
       "      <td>LOC 703 709</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T15</td>\n",
       "      <td>LOC 723 730</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T16</td>\n",
       "      <td>LOC 815 825</td>\n",
       "      <td>Вашингтона</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T17</td>\n",
       "      <td>ORG 838 841</td>\n",
       "      <td>МИД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T18</td>\n",
       "      <td>LOC 842 848</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1                 2\n",
       "0    T1      LOC 0 6            Россия\n",
       "1    T2    LOC 50 53               США\n",
       "2    T3    LOC 57 63            Грузию\n",
       "3    T4    LOC 87 93            МОСКВА\n",
       "4    T5  ORG 103 114       РИА Новости\n",
       "5    T6  LOC 116 122            Россия\n",
       "6    T7  LOC 141 144               США\n",
       "7    T8  LOC 161 168           Тбилиси\n",
       "8    T9  LOC 301 307            России\n",
       "9   T10  PER 308 324  Григорий Карасин\n",
       "10  T11  LOC 383 386               США\n",
       "11  T12  PER 387 402   Дэниэлом Фридом\n",
       "12  T13  LOC 505 517      Южной Осетии\n",
       "13  T14  LOC 703 709            Россия\n",
       "14  T15  LOC 723 730           Тбилиси\n",
       "15  T16  LOC 815 825        Вашингтона\n",
       "16  T17  ORG 838 841               МИД\n",
       "17  T18  LOC 842 848            России"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разметка из collection3\n",
    "pd.read_csv('collection3/001.ann', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** NER из spacy нашла все сущности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. NER из slovnet <a id='section_2.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Россия рассчитывает на конструктивное воздействие США на Грузию\n",
      "LOC───                                            LOC    LOC───\n",
      "04/08/2008 12:08\n",
      "МОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют\n",
      "LOC───          ORG────────  LOC───                   LOC             \n",
      " на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского \n",
      "    LOC────                                                           \n",
      "конфликта. Об этом статс-секретарь - заместитель министра иностранных \n",
      "дел России Григорий Карасин заявил в телефонном разговоре с \n",
      "    LOC─── PER─────────────                                 \n",
      "заместителем госсекретаря США Дэниэлом Фридом.\n",
      "                          LOC PER──────────── \n",
      "\"С российской стороны выражена глубокая озабоченность в связи с новым \n",
      "витком напряженности вокруг Южной Осетии, противозаконными действиями \n",
      "                            LOC─────────                              \n",
      "грузинской стороны по наращиванию своих вооруженных сил в регионе, \n",
      "бесконтрольным строительством фортификационных сооружений\", - \n",
      "говорится в сообщении.\n",
      "\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает \n",
      " LOC───              LOC────                                      \n",
      "также на конструктивное воздействие со стороны Вашингтона\", - сообщил \n",
      "                                               LOC───────             \n",
      "МИД России. \n",
      "ORG LOC───  \n"
     ]
    }
   ],
   "source": [
    "text = data_text.text[0]\n",
    "navec = Navec.load('slovnet/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "ner = NER.load('slovnet/slovnet_ner_news_v1.tar')\n",
    "ner.navec(navec)\n",
    "\n",
    "markup = ner(text)\n",
    "show_markup(markup.text, markup.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** NER из slovnet работает также хорошо, как NER из spacy (нашел все сущности)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Самописный NER <a id='section_2.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.ann', '002.ann', '003.ann', '004.ann', '005.ann', '006.ann', '007.ann', '008.ann', '009.ann', '010.ann', '011.ann', '012.ann', '013.ann', '014.ann', '015 (!).ann', '016.ann', '017.ann', '018.ann', '019.ann', '020.ann', '021.ann', '022.ann', '023.ann', '025.ann', '026.ann', '027.ann', '028.ann', '029.ann', '030.ann', '031.ann', '032.ann', '033.ann', '034.ann', '035.ann', '036.ann', '037.ann', '038.ann', '039.ann', '03_12_12a.ann', '03_12_12b.ann', '03_12_12c.ann', '03_12_12d.ann', '03_12_12g.ann', '03_12_12h.ann', '040.ann', '041.ann', '042.ann', '043.ann', '044.ann', '045.ann', '046.ann', '047.ann', '048.ann', '049.ann', '04_02_13a_abdulatipov.ann', '04_03_13a_sorokin.ann', '04_12_12b.ann', '04_12_12d.ann', '04_12_12f.ann', '04_12_12g.ann', '04_12_12h_corr.ann', '050.ann', '051.ann', '052.ann', '053.ann', '054.ann', '055.ann', '056.ann', '057.ann', '058.ann', '059.ann', '060.ann', '061.ann', '062.ann', '063.ann', '064.ann', '065.ann', '066.ann', '067.ann', '068.ann', '069.ann', '070.ann', '071.ann', '072.ann', '073.ann', '074.ann', '075.ann', '076.ann', '077.ann', '078.ann', '079.ann', '080.ann', '081.ann', '082.ann', '083.ann', '084.ann', '085.ann', '086.ann', '087.ann', '088.ann', '089.ann', '090.ann', '091.ann', '092.ann', '093.ann', '094.ann', '095.ann', '096.ann', '097.ann', '098.ann', '099.ann', '09_01_13.ann', '09_01_13a.ann', '09_01_13c.ann', '09_01_13d.ann', '09_01_13e.ann', '09_01_13h.ann', '09_01_13i.ann', '100.ann', '1000.ann', '1001.ann', '1002.ann', '1003.ann', '1004.ann', '1005.ann', '1006.ann', '1007.ann', '1008.ann', '1009.ann', '101.ann', '1010.ann', '1011.ann', '1012.ann', '1013.ann', '1014.ann', '1015.ann', '1016.ann', '1017.ann', '1018.ann', '1019.ann', '102.ann', '1020.ann', '1021.ann', '1022.ann', '1023.ann', '1024.ann', '1025.ann', '1026.ann', '1027.ann', '1028.ann', '1029.ann', '103.ann', '1030.ann', '1031.ann', '1032.ann', '1033.ann', '1034.ann', '1035.ann', '1036.ann', '1037.ann', '1038.ann', '1039.ann', '104.ann', '1040.ann', '1041.ann', '1042.ann', '1043.ann', '1044.ann', '1045.ann', '1046.ann', '1047.ann', '1048.ann', '1049.ann', '105.ann', '1050.ann', '106.ann', '107.ann', '108.ann', '109.ann', '10_01_13a.ann', '10_01_13d.ann', '10_01_13i.ann', '110.ann', '1100.ann', '1101.ann', '1102.ann', '1103.ann', '1104.ann', '1105.ann', '1106.ann', '1107.ann', '1108.ann', '1109.ann', '111.ann', '1110.ann', '1111.ann', '1112.ann', '1113.ann', '1114.ann', '1115.ann', '1116.ann', '1117.ann', '1118.ann', '1119.ann', '112.ann', '1120.ann', '1121.ann', '1122.ann', '1123.ann', '1124.ann', '1125.ann', '1126.ann', '1127.ann', '1128.ann', '113.ann', '1130.ann', '1131.ann', '1132.ann', '1133.ann', '1134.ann', '1135.ann', '1136.ann', '1137.ann', '1138.ann', '1139.ann', '114.ann', '1140.ann', '1141.ann', '1142.ann', '1143.ann', '1144.ann', '1145.ann', '1146.ann', '1147.ann', '1148.ann', '1149.ann', '115.ann', '1150.ann', '1151.ann', '1152.ann', '1153.ann', '1154.ann', '1155.ann', '1156.ann', '1157.ann', '1158.ann', '1159.ann', '116.ann', '1160.ann', '1161.ann', '1162.ann', '1163.ann', '1164.ann', '1165.ann', '1166.ann', '1167.ann', '1168.ann', '1169.ann', '117.ann', '1170.ann', '1171.ann', '1172.ann', '1173.ann', '1174.ann', '1175.ann', '1176.ann', '1177.ann', '1178.ann', '1179.ann', '118.ann', '1180.ann', '1181.ann', '1182.ann', '1183.ann', '1184.ann', '1185.ann', '1186.ann', '1187.ann', '1188.ann', '1189.ann', '119.ann', '1190.ann', '1191.ann', '1192.ann', '1193.ann', '1194.ann', '1195.ann', '1196.ann', '1197.ann', '1198.ann', '1199.ann', '11_01_13b.ann', '11_01_13e.ann', '120.ann', '1200.ann', '121.ann', '122.ann', '123.ann', '124.ann', '125.ann', '126.ann', '127.ann', '128.ann', '129.ann', '130.ann', '131.ann', '132.ann', '133.ann', '134.ann', '135.ann', '136.ann', '137.ann', '138.ann', '139.ann', '140.ann', '141.ann', '142.ann', '143.ann', '144.ann', '145.ann', '146.ann', '147.ann', '148.ann', '149.ann', '14_01_13c.ann', '14_01_13g.ann', '14_01_13i.ann', '150.ann', '151.ann', '152.ann', '153.ann', '154.ann', '155.ann', '156.ann', '157.ann', '158.ann', '159.ann', '15_01_13a.ann', '15_01_13b.ann', '15_01_13e.ann', '15_01_13f.ann', '160.ann', '161.ann', '162.ann', '163.ann', '164.ann', '165.ann', '166.ann', '167.ann', '168.ann', '169.ann', '170.ann', '171.ann', '172.ann', '173.ann', '174.ann', '175.ann', '176.ann', '177.ann', '178.ann', '179.ann', '180.ann', '181.ann', '182.ann', '183.ann', '184.ann', '185.ann', '186.ann', '187.ann', '188.ann', '189.ann', '190.ann', '191.ann', '192.ann', '193.ann', '194.ann', '195.ann', '196.ann', '197.ann', '198.ann', '199.ann', '19_11_12d.ann', '19_11_12h.ann', '200.ann', '2001.ann', '2002.ann', '2003.ann', '2004.ann', '2005.ann', '2006.ann', '2007.ann', '2008.ann', '2009.ann', '201.ann', '2010.ann', '2011.ann', '2012.ann', '2013.ann', '2014.ann', '2015.ann', '2016.ann', '2017.ann', '2018.ann', '2019.ann', '202.ann', '2020.ann', '2021.ann', '2022.ann', '2023.ann', '2024.ann', '2025.ann', '2026.ann', '2027.ann', '2028.ann', '2029.ann', '203.ann', '2030.ann', '2031.ann', '2032.ann', '2034.ann', '2035.ann', '2036.ann', '2037.ann', '2038.ann', '2039.ann', '204.ann', '2040.ann', '2041.ann', '2042.ann', '2043.ann', '2044.ann', '2045.ann', '2046.ann', '2047.ann', '2048.ann', '2049.ann', '205.ann', '2050.ann', '206.ann', '207.ann', '208.ann', '209.ann', '20_11_12a.ann', '20_11_12b.ann', '20_11_12c.ann', '20_11_12d.ann', '20_11_12i.ann', '210.ann', '211.ann', '212.ann', '213.ann', '214.ann', '215.ann', '216.ann', '217.ann', '218.ann', '219.ann', '21_11_12c.ann', '21_11_12h.ann', '21_11_12i.ann', '21_11_12j.ann', '220.ann', '221.ann', '222.ann', '223.ann', '224.ann', '225.ann', '226.ann', '227.ann', '228.ann', '229.ann', '22_11_12a.ann', '22_11_12c.ann', '22_11_12d.ann', '22_11_12g.ann', '22_11_12h.ann', '22_11_12i.ann', '22_11_12j.ann', '230.ann', '231.ann', '232.ann', '233.ann', '234.ann', '235.ann', '236.ann', '237.ann', '238.ann', '239.ann', '23_11_12a.ann', '23_11_12b.ann', '23_11_12c.ann', '23_11_12d.ann', '23_11_12e.ann', '23_11_12f.ann', '240.ann', '241.ann', '242.ann', '243.ann', '244.ann', '245.ann', '246.ann', '247.ann', '248.ann', '249.ann', '250.ann', '251.ann', '252.ann', '253.ann', '254.ann', '255.ann', '256.ann', '257.ann', '258.ann', '259.ann', '25_12_12a.ann', '25_12_12c.ann', '25_12_12d.ann', '25_12_12e.ann', '260.ann', '261.ann', '262.ann', '263.ann', '264.ann', '265.ann', '266.ann', '267.ann', '268.ann', '269.ann', '26_11_12b.ann', '26_11_12c.ann', '26_11_12e.ann', '26_11_12f.ann', '270.ann', '271.ann', '272.ann', '273.ann', '274.ann', '275.ann', '276.ann', '277.ann', '278.ann', '279.ann', '27_11_12a.ann', '27_11_12c.ann', '27_11_12d.ann', '27_11_12e.ann', '27_11_12j.ann', '280.ann', '281.ann', '282.ann', '283.ann', '284.ann', '285.ann', '286.ann', '287.ann', '288.ann', '289.ann', '28_11_12a.ann', '28_11_12f.ann', '28_11_12g.ann', '28_11_12h.ann', '28_11_12i.ann', '28_11_12j.ann', '290.ann', '291.ann', '292.ann', '293.ann', '294.ann', '295.ann', '296.ann', '297.ann', '298.ann', '299.ann', '29_11_12a.ann', '29_11_12b.ann', '300.ann', '301.ann', '302.ann', '303.ann', '304.ann', '305.ann', '306.ann', '307.ann', '308.ann', '309.ann', '30_11_12b.ann', '30_11_12h.ann', '30_11_12i.ann', '310.ann', '311.ann', '312.ann', '313.ann', '314.ann', '315.ann', '316.ann', '317.ann', '318.ann', '319.ann', '320.ann', '321.ann', '322.ann', '323.ann', '324.ann', '325.ann', '326.ann', '327.ann', '328.ann', '329.ann', '330.ann', '331.ann', '332.ann', '333.ann', '334.ann', '335.ann', '336.ann', '337.ann', '338.ann', '339.ann', '340.ann', '341.ann', '342.ann', '343.ann', '344.ann', '345.ann', '346.ann', '347.ann', '348.ann', '349.ann', '350.ann', '351.ann', '352.ann', '353.ann', '354.ann', '355.ann', '356.ann', '357.ann', '358.ann', '359.ann', '360.ann', '361.ann', '362.ann', '363.ann', '364.ann', '365.ann', '366.ann', '367.ann', '368.ann', '369.ann', '370.ann', '371.ann', '372.ann', '373.ann', '374.ann', '375.ann', '376.ann', '377.ann', '378.ann', '379.ann', '380.ann', '381.ann', '382.ann', '383.ann', '384.ann', '385.ann', '386.ann', '387.ann', '388.ann', '389.ann', '390.ann', '391.ann', '392.ann', '393.ann', '394.ann', '395.ann', '396.ann', '397.ann', '398.ann', '399.ann', '400.ann', '401.ann', '402.ann', '403.ann', '404.ann', '405.ann', '406.ann', '407.ann', '408.ann', '409.ann', '410.ann', '411.ann', '412.ann', '413.ann', '414.ann', '415.ann', '416.ann', '417.ann', '418.ann', '419.ann', '420.ann', '421.ann', '422.ann', '423.ann', '424.ann', '425.ann', '426.ann', '427.ann', '428.ann', '429.ann', '430.ann', '431.ann', '432.ann', '433.ann', '434.ann', '435.ann', '436.ann', '437.ann', '438.ann', '439.ann', '440.ann', '441.ann', '442.ann', '443.ann', '444.ann', '445.ann', '446.ann', '447.ann', '448.ann', '449.ann', '450.ann', '451.ann', '452.ann', '453.ann', '454.ann', '455.ann', '457.ann', '458.ann', '459.ann', '460.ann', '461.ann', '462.ann', '463.ann', '464.ann', '465.ann', '466.ann', '467.ann', '468.ann', '469.ann', '470.ann', '471.ann', '472.ann', '473.ann', '474.ann', '475.ann', '476.ann', '477.ann', '478.ann', '479.ann', '480.ann', '481.ann', '482.ann', '483.ann', '484.ann', '485.ann', '486.ann', '487.ann', '488.ann', '489.ann', '490.ann', '491.ann', '492.ann', '493.ann', '494.ann', '495.ann', '496.ann', '497.ann', '498.ann', '499.ann', '500.ann', '501.ann', '502.ann', '503.ann', '504.ann', '505.ann', '506.ann', '507.ann', '508.ann', '509.ann', '510.ann', '511.ann', '512.ann', '513.ann', '514.ann', '515.ann', '516.ann', '517.ann', '518.ann', '519.ann', '520.ann', '521.ann', '522.ann', '523.ann', '524.ann', '525.ann', '526.ann', '527.ann', '528.ann', '529.ann', '530.ann', '531.ann', '532.ann', '533 (!).ann', '534.ann', '535.ann', '536.ann', '537.ann', '538.ann', '539.ann', '540.ann', '541.ann', '542.ann', '543.ann', '544.ann', '545.ann', '546.ann', '547.ann', '548.ann', '549.ann', '550.ann', '551.ann', '552.ann', '553.ann', '554.ann', '555 (!).ann', '556.ann', '557.ann', '558.ann', '559.ann', '560.ann', '561.ann', '562.ann', '563.ann', '564.ann', '565.ann', '567.ann', '568.ann', '569.ann', '570.ann', '571.ann', '572.ann', '574.ann', '575.ann', '576.ann', '577.ann', '578.ann', '579.ann', '581.ann', '582.ann', '583.ann', '584 (!).ann', '585.ann', '586.ann', '587.ann', '588.ann', '589.ann', '590.ann', '591.ann', '592.ann', '593.ann', '594.ann', '596.ann', '597.ann', '598 (!).ann', '599.ann', '600.ann', '601.ann', '602.ann', '610.ann', '611.ann', '612.ann', '613.ann', '614.ann', '615.ann', '616.ann', '617.ann', '618.ann', '619.ann', '620.ann', '621.ann', '622.ann', '623.ann', '624.ann', '625.ann', '626.ann', '627.ann', '628.ann', '629.ann', '630.ann', '631.ann', '632.ann', '633.ann', 'abdulatipov.ann', 'artjakov.ann', 'Avtovaz.ann', 'blokhin.ann', 'chaves.ann', 'chirkunov.ann', 'kamchatka.ann', 'klinton.ann', 'kuleshov.ann', 'last_01.ann', 'last_02.ann', 'last_03.ann', 'last_04.ann', 'last_05.ann', 'last_06.ann', 'last_07_new.ann', 'last_08.ann', 'last_09.ann', 'last_10.ann', 'last_11.ann', 'last_12.ann', 'last_13.ann', 'last_14.ann', 'last_15.ann', 'last_16.ann', 'last_17.ann', 'last_18.ann', 'last_19.ann', 'last_20.ann', 'last_21.ann', 'last_22.ann', 'last_23.ann', 'last_24.ann', 'last_25.ann', 'last_26.ann', 'last_27.ann', 'last_28.ann', 'last_29.ann', 'last_30_new.ann', 'last_31.ann', 'last_32.ann', 'last_33.ann', 'last_34.ann', 'last_35.ann', 'last_36.ann', 'last_37.ann', 'last_38.ann', 'last_39.ann', 'last_40.ann', 'last_41.ann', 'last_42.ann', 'last_43.ann', 'last_44.ann', 'last_45.ann', 'last_46.ann', 'last_47.ann', 'last_48.ann', 'last_49.ann', 'last_50.ann', 'last_51.ann', 'last_52.ann', 'last_53.ann', 'last_54.ann', 'last_55.ann', 'last_56.ann', 'last_57.ann', 'last_58.ann', 'last_59.ann', 'last_60.ann', 'last_61.ann', 'last_62.ann', 'last_63.ann', 'last_64.ann', 'last_65.ann', 'last_66.ann', 'last_67.ann', 'last_68.ann', 'last_69.ann', 'last_70.ann', 'last_71.ann', 'last_72.ann', 'last_73.ann', 'last_74.ann', 'last_75.ann', 'lenoblast.ann', 'maykl dzhekson.ann', 'mvd.ann', 'mvd2.ann', 'rosobrnadzor.ann', 'ryadovoy chelah.ann', 'semenenko.ann', 'shojgu1.ann', 'shojgu3.ann', 'shojgu4.ann', 'shojgu6.ann', 'si_tzjanpin.ann', 'sobjanin2.ann', 'turkmenija.ann', 'uchitel.ann']\n"
     ]
    }
   ],
   "source": [
    "#собираем только ann файлы коллекции\n",
    "fileDir = r\"collection3\"\n",
    "fileExt = r\".ann\"\n",
    "documents_ann = [_ for _ in os.listdir(fileDir) if _.endswith(fileExt)]\n",
    "print(documents_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>LOC 0 6</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>LOC 50 53</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>LOC 57 63</td>\n",
       "      <td>Грузию</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>LOC 87 93</td>\n",
       "      <td>МОСКВА</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>ORG 103 114</td>\n",
       "      <td>РИА Новости</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T6</td>\n",
       "      <td>LOC 116 122</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T7</td>\n",
       "      <td>LOC 141 144</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T8</td>\n",
       "      <td>LOC 161 168</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T9</td>\n",
       "      <td>LOC 301 307</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T10</td>\n",
       "      <td>PER 308 324</td>\n",
       "      <td>Григорий Карасин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T11</td>\n",
       "      <td>LOC 383 386</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T12</td>\n",
       "      <td>PER 387 402</td>\n",
       "      <td>Дэниэлом Фридом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T13</td>\n",
       "      <td>LOC 505 517</td>\n",
       "      <td>Южной Осетии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T14</td>\n",
       "      <td>LOC 703 709</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T15</td>\n",
       "      <td>LOC 723 730</td>\n",
       "      <td>Тбилиси</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T16</td>\n",
       "      <td>LOC 815 825</td>\n",
       "      <td>Вашингтона</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T17</td>\n",
       "      <td>ORG 838 841</td>\n",
       "      <td>МИД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T18</td>\n",
       "      <td>LOC 842 848</td>\n",
       "      <td>России</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1                 2\n",
       "0    T1      LOC 0 6            Россия\n",
       "1    T2    LOC 50 53               США\n",
       "2    T3    LOC 57 63            Грузию\n",
       "3    T4    LOC 87 93            МОСКВА\n",
       "4    T5  ORG 103 114       РИА Новости\n",
       "5    T6  LOC 116 122            Россия\n",
       "6    T7  LOC 141 144               США\n",
       "7    T8  LOC 161 168           Тбилиси\n",
       "8    T9  LOC 301 307            России\n",
       "9   T10  PER 308 324  Григорий Карасин\n",
       "10  T11  LOC 383 386               США\n",
       "11  T12  PER 387 402   Дэниэлом Фридом\n",
       "12  T13  LOC 505 517      Южной Осетии\n",
       "13  T14  LOC 703 709            Россия\n",
       "14  T15  LOC 723 730           Тбилиси\n",
       "15  T16  LOC 815 825        Вашингтона\n",
       "16  T17  ORG 838 841               МИД\n",
       "17  T18  LOC 842 848            России"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = pd.read_csv('collection3/001.ann', delimiter='\\t', header=None)\n",
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "#составляем списки токенов и интенсов (из файла .ann делается словарь {слово : интенс}, из словаря каждому токену сопоствляем интенс)\n",
    "docs = []\n",
    "for i in range(len(data_text)):\n",
    "    words = []\n",
    "    labels = []\n",
    "    #подготавливаем текст\n",
    "    text = data_text['text'][i]\n",
    "    \n",
    "    df = pd.read_csv('collection3/' + documents_ann[i], delimiter='\\t', header=None)\n",
    "    df_ann = pd.DataFrame()\n",
    "    df_ann['Token'] = df.loc[:, 2]\n",
    "    split_1 = [loc.split() for loc in df.loc[:, 1].values]\n",
    "    df_ann['Entity'] = [loc[0] for loc in split_1]\n",
    "       \n",
    "    dic = {}\n",
    "    for j in range(len(df)):\n",
    "        token = df_ann['Token'][j].lower().split()\n",
    "        entity = df_ann['Entity'][j]\n",
    "        for tok in token:\n",
    "            dic[tok] = entity\n",
    "\n",
    "    for token in tokenize(text):\n",
    "        if (token.text.lower() in dic.keys()):\n",
    "            words.append(token.text)\n",
    "            labels.append(dic[token.text.lower()])\n",
    "        else:\n",
    "            words.append(token.text)\n",
    "            labels.append('OUT')\n",
    "    \n",
    "    docs.append([words, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Россия', 'рассчитывает', 'на', 'конструктивное', 'воздействие', 'США', 'на', 'Грузию', '04/08/2008', '12', ':', '08', 'МОСКВА', ',', '4', 'авг', '-', 'РИА', 'Новости', '.', 'Россия', 'рассчитывает', ',', 'что', 'США', 'воздействуют', 'на', 'Тбилиси', 'в', 'связи', 'с', 'обострением', 'ситуации', 'в', 'зоне', 'грузино-осетинского', 'конфликта', '.', 'Об', 'этом', 'статс-секретарь', '-', 'заместитель', 'министра', 'иностранных', 'дел', 'России', 'Григорий', 'Карасин', 'заявил', 'в', 'телефонном', 'разговоре', 'с', 'заместителем', 'госсекретаря', 'США', 'Дэниэлом', 'Фридом', '.', '\"', 'С', 'российской', 'стороны', 'выражена', 'глубокая', 'озабоченность', 'в', 'связи', 'с', 'новым', 'витком', 'напряженности', 'вокруг', 'Южной', 'Осетии', ',', 'противозаконными', 'действиями', 'грузинской', 'стороны', 'по', 'наращиванию', 'своих', 'вооруженных', 'сил', 'в', 'регионе', ',', 'бесконтрольным', 'строительством', 'фортификационных', 'сооружений', '\"', ',', '-', 'говорится', 'в', 'сообщении', '.', '\"', 'Россия', 'уже', 'призвала', 'Тбилиси', 'к', 'ответственной', 'линии', 'и', 'рассчитывает', 'также', 'на', 'конструктивное', 'воздействие', 'со', 'стороны', 'Вашингтона', '\"', ',', '-', 'сообщил', 'МИД', 'России', '.']\n",
      "['LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'ORG', 'ORG', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'PER', 'PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'PER', 'PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'ORG', 'LOC', 'OUT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs[0][0]), print(docs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Россия\tLOC\n",
      "рассчитывает\tOUT\n",
      "на\tOUT\n",
      "конструктивное\tOUT\n",
      "воздействие\tOUT\n",
      "США\tLOC\n",
      "на\tOUT\n",
      "Грузию\tLOC\n",
      "04/08/2008\tOUT\n",
      "12\tOUT\n",
      ":\tOUT\n",
      "08\tOUT\n",
      "МОСКВА\tLOC\n",
      ",\tOUT\n",
      "4\tOUT\n",
      "авг\tOUT\n",
      "-\tOUT\n",
      "РИА\tORG\n",
      "Новости\tORG\n",
      ".\tOUT\n",
      "Россия\tLOC\n",
      "рассчитывает\tOUT\n",
      ",\tOUT\n",
      "что\tOUT\n",
      "США\tLOC\n",
      "воздействуют\tOUT\n",
      "на\tOUT\n",
      "Тбилиси\tLOC\n",
      "в\tOUT\n",
      "связи\tOUT\n",
      "с\tOUT\n",
      "обострением\tOUT\n",
      "ситуации\tOUT\n",
      "в\tOUT\n",
      "зоне\tOUT\n",
      "грузино-осетинского\tOUT\n",
      "конфликта\tOUT\n",
      ".\tOUT\n",
      "Об\tOUT\n",
      "этом\tOUT\n",
      "статс-секретарь\tOUT\n",
      "-\tOUT\n",
      "заместитель\tOUT\n",
      "министра\tOUT\n",
      "иностранных\tOUT\n",
      "дел\tOUT\n",
      "России\tLOC\n",
      "Григорий\tPER\n",
      "Карасин\tPER\n",
      "заявил\tOUT\n",
      "в\tOUT\n",
      "телефонном\tOUT\n",
      "разговоре\tOUT\n",
      "с\tOUT\n",
      "заместителем\tOUT\n",
      "госсекретаря\tOUT\n",
      "США\tLOC\n",
      "Дэниэлом\tPER\n",
      "Фридом\tPER\n",
      ".\tOUT\n",
      "\"\tOUT\n",
      "С\tOUT\n",
      "российской\tOUT\n",
      "стороны\tOUT\n",
      "выражена\tOUT\n",
      "глубокая\tOUT\n",
      "озабоченность\tOUT\n",
      "в\tOUT\n",
      "связи\tOUT\n",
      "с\tOUT\n",
      "новым\tOUT\n",
      "витком\tOUT\n",
      "напряженности\tOUT\n",
      "вокруг\tOUT\n",
      "Южной\tLOC\n",
      "Осетии\tLOC\n",
      ",\tOUT\n",
      "противозаконными\tOUT\n",
      "действиями\tOUT\n",
      "грузинской\tOUT\n",
      "стороны\tOUT\n",
      "по\tOUT\n",
      "наращиванию\tOUT\n",
      "своих\tOUT\n",
      "вооруженных\tOUT\n",
      "сил\tOUT\n",
      "в\tOUT\n",
      "регионе\tOUT\n",
      ",\tOUT\n",
      "бесконтрольным\tOUT\n",
      "строительством\tOUT\n",
      "фортификационных\tOUT\n",
      "сооружений\tOUT\n",
      "\"\tOUT\n",
      ",\tOUT\n",
      "-\tOUT\n",
      "говорится\tOUT\n",
      "в\tOUT\n",
      "сообщении\tOUT\n",
      ".\tOUT\n",
      "\"\tOUT\n",
      "Россия\tLOC\n",
      "уже\tOUT\n",
      "призвала\tOUT\n",
      "Тбилиси\tLOC\n",
      "к\tOUT\n",
      "ответственной\tOUT\n",
      "линии\tOUT\n",
      "и\tOUT\n",
      "рассчитывает\tOUT\n",
      "также\tOUT\n",
      "на\tOUT\n",
      "конструктивное\tOUT\n",
      "воздействие\tOUT\n",
      "со\tOUT\n",
      "стороны\tOUT\n",
      "Вашингтона\tLOC\n",
      "\"\tOUT\n",
      ",\tOUT\n",
      "-\tOUT\n",
      "сообщил\tOUT\n",
      "МИД\tORG\n",
      "России\tLOC\n",
      ".\tOUT\n"
     ]
    }
   ],
   "source": [
    "data, labels = list(zip(*docs))\n",
    "for w, e in zip(data[0], labels[0]):\n",
    "    print(f'{w}\\t{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оболочка **sklearn_crfsuite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>data</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Россия</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>рассчитывает</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>конструктивное</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>воздействие</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>США</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Грузию</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>04/08/2008</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>:</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>08</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>МОСКВА</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>авг</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>РИА</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Новости</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Россия</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>рассчитывает</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>что</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>США</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>воздействуют</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Тбилиси</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>связи</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>с</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>обострением</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>ситуации</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>зоне</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>грузино-осетинского</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>конфликта</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>Об</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>этом</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>статс-секретарь</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>заместитель</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>министра</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>иностранных</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>дел</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>России</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>Григорий</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>Карасин</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>заявил</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id                 data entities\n",
       "0         0               Россия      LOC\n",
       "1         0         рассчитывает      OUT\n",
       "2         0                   на      OUT\n",
       "3         0       конструктивное      OUT\n",
       "4         0          воздействие      OUT\n",
       "5         0                  США      LOC\n",
       "6         0                   на      OUT\n",
       "7         0               Грузию      LOC\n",
       "8         0           04/08/2008      OUT\n",
       "9         0                   12      OUT\n",
       "10        0                    :      OUT\n",
       "11        0                   08      OUT\n",
       "12        0               МОСКВА      LOC\n",
       "13        0                    ,      OUT\n",
       "14        0                    4      OUT\n",
       "15        0                  авг      OUT\n",
       "16        0                    -      OUT\n",
       "17        0                  РИА      ORG\n",
       "18        0              Новости      ORG\n",
       "19        0                    .      OUT\n",
       "20        0               Россия      LOC\n",
       "21        0         рассчитывает      OUT\n",
       "22        0                    ,      OUT\n",
       "23        0                  что      OUT\n",
       "24        0                  США      LOC\n",
       "25        0         воздействуют      OUT\n",
       "26        0                   на      OUT\n",
       "27        0              Тбилиси      LOC\n",
       "28        0                    в      OUT\n",
       "29        0                связи      OUT\n",
       "30        0                    с      OUT\n",
       "31        0          обострением      OUT\n",
       "32        0             ситуации      OUT\n",
       "33        0                    в      OUT\n",
       "34        0                 зоне      OUT\n",
       "35        0  грузино-осетинского      OUT\n",
       "36        0            конфликта      OUT\n",
       "37        0                    .      OUT\n",
       "38        0                   Об      OUT\n",
       "39        0                 этом      OUT\n",
       "40        0      статс-секретарь      OUT\n",
       "41        0                    -      OUT\n",
       "42        0          заместитель      OUT\n",
       "43        0             министра      OUT\n",
       "44        0          иностранных      OUT\n",
       "45        0                  дел      OUT\n",
       "46        0               России      LOC\n",
       "47        0             Григорий      PER\n",
       "48        0              Карасин      PER\n",
       "49        0               заявил      OUT"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s) for i, s in enumerate(data)] for i in j],\n",
    "                   'data': [i for j in data for i in j],\n",
    "                   'entities': [i for j in labels for i in j]})\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Будем передавать в модель токен с соседями**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразуем датасет: \n",
    "#группируем записи по номеру предложения (аггфункция - список кортежей (токен, лейбл))\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n",
    "                                                           s['entities'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "#преобразует датасет df\n",
    "getter = SentenceGetter(df)\n",
    "#получаем список номеров преложений датасета df\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразуем слова в признаки:\n",
    "#для каждого слова-токена составляем словарь признаков, \n",
    "#определяем является ли оно начаотным, конечным в предложении\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "#преобразуем предложения:\n",
    "#каждое слово предложения преобразуем в признаки\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "#список всех лейблов предложения\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "#список всех токенов предложения\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем данные и метки\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'в',\n",
       " 'word[-3:]': 'в',\n",
       " 'word[-2:]': 'в',\n",
       " 'word.isdigit()': False,\n",
       " '-1:word.lower()': 'изъяты',\n",
       " '+1:word.lower()': 'арендуемом'}"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#передаем в датасет токен с соседями (первый индекс -- предложение, 2 индекс -- слово)\n",
    "X[2][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем данные на train и test\n",
    "X_train = X[:700]\n",
    "X_test = X[700:]\n",
    "y_train = y[:700]\n",
    "y_test = y[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|████████████████████████████████████████████| 700/700 [00:01<00:00, 579.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 98972\n",
      "Seconds required: 0.344\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 1000\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.12  loss=121356.43 active=98097 feature_norm=1.00\n",
      "Iter 2   time=0.06  loss=108933.37 active=94193 feature_norm=1.59\n",
      "Iter 3   time=0.06  loss=106719.02 active=97471 feature_norm=1.47\n",
      "Iter 4   time=0.06  loss=105586.47 active=97173 feature_norm=1.40\n",
      "Iter 5   time=0.06  loss=103864.92 active=97820 feature_norm=1.46\n",
      "Iter 6   time=0.06  loss=88408.59 active=97567 feature_norm=4.93\n",
      "Iter 7   time=0.06  loss=86020.46 active=98489 feature_norm=4.83\n",
      "Iter 8   time=0.06  loss=82185.93 active=98955 feature_norm=5.16\n",
      "Iter 9   time=0.11  loss=78184.62 active=98904 feature_norm=6.24\n",
      "Iter 10  time=0.06  loss=75887.88 active=98782 feature_norm=6.89\n",
      "Iter 11  time=0.06  loss=69572.49 active=97639 feature_norm=10.35\n",
      "Iter 12  time=0.06  loss=65445.13 active=95754 feature_norm=12.05\n",
      "Iter 13  time=0.06  loss=61179.94 active=93953 feature_norm=15.62\n",
      "Iter 14  time=0.06  loss=55685.16 active=86236 feature_norm=20.45\n",
      "Iter 15  time=0.06  loss=51267.20 active=84206 feature_norm=24.92\n",
      "Iter 16  time=0.06  loss=48883.76 active=82984 feature_norm=27.25\n",
      "Iter 17  time=0.06  loss=46042.60 active=81979 feature_norm=31.01\n",
      "Iter 18  time=0.06  loss=42059.06 active=80413 feature_norm=37.66\n",
      "Iter 19  time=0.06  loss=39428.75 active=77923 feature_norm=44.30\n",
      "Iter 20  time=0.06  loss=36914.03 active=77586 feature_norm=48.65\n",
      "Iter 21  time=0.06  loss=34224.40 active=76774 feature_norm=56.09\n",
      "Iter 22  time=0.06  loss=31282.40 active=73511 feature_norm=66.99\n",
      "Iter 23  time=0.06  loss=28877.76 active=71845 feature_norm=76.51\n",
      "Iter 24  time=0.06  loss=26374.33 active=68252 feature_norm=89.34\n",
      "Iter 25  time=0.06  loss=24782.24 active=66437 feature_norm=106.72\n",
      "Iter 26  time=0.06  loss=23165.97 active=65648 feature_norm=111.55\n",
      "Iter 27  time=0.06  loss=22137.71 active=65377 feature_norm=118.13\n",
      "Iter 28  time=0.07  loss=20709.46 active=64182 feature_norm=129.00\n",
      "Iter 29  time=0.06  loss=19394.61 active=61514 feature_norm=139.14\n",
      "Iter 30  time=0.06  loss=18641.38 active=61207 feature_norm=145.62\n",
      "Iter 31  time=0.06  loss=17967.82 active=58832 feature_norm=153.10\n",
      "Iter 32  time=0.06  loss=17110.50 active=57813 feature_norm=162.66\n",
      "Iter 33  time=0.12  loss=16977.05 active=57154 feature_norm=165.90\n",
      "Iter 34  time=0.06  loss=16569.95 active=56864 feature_norm=167.53\n",
      "Iter 35  time=0.06  loss=16317.44 active=55592 feature_norm=170.34\n",
      "Iter 36  time=0.06  loss=15901.91 active=53211 feature_norm=175.07\n",
      "Iter 37  time=0.06  loss=15755.69 active=51332 feature_norm=176.57\n",
      "Iter 38  time=0.06  loss=15589.88 active=51419 feature_norm=177.20\n",
      "Iter 39  time=0.06  loss=15496.48 active=50401 feature_norm=178.60\n",
      "Iter 40  time=0.06  loss=15318.61 active=48811 feature_norm=182.51\n",
      "Iter 41  time=0.07  loss=15226.91 active=48454 feature_norm=183.47\n",
      "Iter 42  time=0.06  loss=15144.21 active=47532 feature_norm=185.33\n",
      "Iter 43  time=0.06  loss=15059.07 active=45871 feature_norm=187.02\n",
      "Iter 44  time=0.06  loss=14994.45 active=45451 feature_norm=187.73\n",
      "Iter 45  time=0.06  loss=14922.02 active=43889 feature_norm=188.84\n",
      "Iter 46  time=0.17  loss=14912.25 active=43033 feature_norm=189.68\n",
      "Iter 47  time=0.06  loss=14846.81 active=42773 feature_norm=189.80\n",
      "Iter 48  time=0.06  loss=14801.42 active=42376 feature_norm=190.59\n",
      "Iter 49  time=0.06  loss=14797.00 active=41412 feature_norm=192.61\n",
      "Iter 50  time=0.06  loss=14725.61 active=41435 feature_norm=193.00\n",
      "Iter 51  time=0.06  loss=14707.90 active=41430 feature_norm=192.89\n",
      "Iter 52  time=0.06  loss=14681.36 active=41170 feature_norm=192.90\n",
      "Iter 53  time=0.06  loss=14648.40 active=40629 feature_norm=192.91\n",
      "Iter 54  time=0.06  loss=14623.78 active=40611 feature_norm=192.77\n",
      "Iter 55  time=0.06  loss=14598.29 active=40246 feature_norm=192.89\n",
      "Iter 56  time=0.06  loss=14569.70 active=39432 feature_norm=193.18\n",
      "Iter 57  time=0.06  loss=14550.45 active=39351 feature_norm=193.23\n",
      "Iter 58  time=0.06  loss=14530.74 active=39178 feature_norm=193.44\n",
      "Iter 59  time=0.06  loss=14510.10 active=38912 feature_norm=193.65\n",
      "Iter 60  time=0.06  loss=14492.94 active=38761 feature_norm=193.81\n",
      "Iter 61  time=0.06  loss=14481.69 active=38617 feature_norm=193.87\n",
      "Iter 62  time=0.06  loss=14467.40 active=38339 feature_norm=194.09\n",
      "Iter 63  time=0.06  loss=14455.89 active=38221 feature_norm=194.11\n",
      "Iter 64  time=0.06  loss=14445.85 active=38128 feature_norm=194.30\n",
      "Iter 65  time=0.06  loss=14435.52 active=38000 feature_norm=194.35\n",
      "Iter 66  time=0.06  loss=14426.00 active=37795 feature_norm=194.55\n",
      "Iter 67  time=0.06  loss=14418.35 active=37771 feature_norm=194.54\n",
      "Iter 68  time=0.06  loss=14411.37 active=37717 feature_norm=194.64\n",
      "Iter 69  time=0.06  loss=14403.92 active=37639 feature_norm=194.65\n",
      "Iter 70  time=0.07  loss=14397.55 active=37531 feature_norm=194.72\n",
      "Iter 71  time=0.06  loss=14392.08 active=37450 feature_norm=194.68\n",
      "Iter 72  time=0.06  loss=14386.49 active=37371 feature_norm=194.72\n",
      "Iter 73  time=0.06  loss=14381.22 active=37282 feature_norm=194.69\n",
      "Iter 74  time=0.06  loss=14376.28 active=37226 feature_norm=194.69\n",
      "Iter 75  time=0.06  loss=14371.49 active=37139 feature_norm=194.63\n",
      "Iter 76  time=0.06  loss=14367.14 active=37075 feature_norm=194.63\n",
      "Iter 77  time=0.06  loss=14363.48 active=37020 feature_norm=194.58\n",
      "Iter 78  time=0.06  loss=14359.95 active=36971 feature_norm=194.59\n",
      "Iter 79  time=0.06  loss=14356.33 active=36947 feature_norm=194.51\n",
      "Iter 80  time=0.06  loss=14352.80 active=36891 feature_norm=194.50\n",
      "Iter 81  time=0.06  loss=14349.72 active=36869 feature_norm=194.43\n",
      "Iter 82  time=0.06  loss=14346.82 active=36842 feature_norm=194.41\n",
      "Iter 83  time=0.06  loss=14343.80 active=36815 feature_norm=194.34\n",
      "Iter 84  time=0.06  loss=14341.12 active=36774 feature_norm=194.34\n",
      "Iter 85  time=0.06  loss=14338.49 active=36763 feature_norm=194.29\n",
      "Iter 86  time=0.06  loss=14336.27 active=36741 feature_norm=194.30\n",
      "Iter 87  time=0.06  loss=14333.91 active=36738 feature_norm=194.26\n",
      "Iter 88  time=0.06  loss=14331.86 active=36694 feature_norm=194.27\n",
      "Iter 89  time=0.06  loss=14329.68 active=36708 feature_norm=194.23\n",
      "Iter 90  time=0.06  loss=14327.93 active=36669 feature_norm=194.26\n",
      "Iter 91  time=0.06  loss=14325.89 active=36659 feature_norm=194.21\n",
      "Iter 92  time=0.06  loss=14324.30 active=36648 feature_norm=194.23\n",
      "Iter 93  time=0.06  loss=14322.42 active=36631 feature_norm=194.20\n",
      "Iter 94  time=0.06  loss=14320.91 active=36594 feature_norm=194.20\n",
      "Iter 95  time=0.06  loss=14319.16 active=36559 feature_norm=194.16\n",
      "Iter 96  time=0.06  loss=14317.78 active=36533 feature_norm=194.17\n",
      "Iter 97  time=0.06  loss=14316.18 active=36524 feature_norm=194.13\n",
      "Iter 98  time=0.06  loss=14314.84 active=36495 feature_norm=194.15\n",
      "Iter 99  time=0.06  loss=14313.46 active=36466 feature_norm=194.11\n",
      "Iter 100 time=0.06  loss=14312.43 active=36442 feature_norm=194.14\n",
      "Iter 101 time=0.06  loss=14310.91 active=36433 feature_norm=194.12\n",
      "Iter 102 time=0.06  loss=14309.61 active=36431 feature_norm=194.15\n",
      "Iter 103 time=0.06  loss=14308.46 active=36422 feature_norm=194.14\n",
      "Iter 104 time=0.06  loss=14307.74 active=36395 feature_norm=194.17\n",
      "Iter 105 time=0.06  loss=14306.84 active=36385 feature_norm=194.16\n",
      "Iter 106 time=0.06  loss=14305.46 active=36382 feature_norm=194.19\n",
      "Iter 107 time=0.06  loss=14304.52 active=36394 feature_norm=194.18\n",
      "Iter 108 time=0.06  loss=14303.59 active=36373 feature_norm=194.20\n",
      "Iter 109 time=0.06  loss=14302.65 active=36350 feature_norm=194.19\n",
      "Iter 110 time=0.06  loss=14301.54 active=36341 feature_norm=194.23\n",
      "Iter 111 time=0.06  loss=14300.98 active=36339 feature_norm=194.22\n",
      "Iter 112 time=0.06  loss=14299.87 active=36340 feature_norm=194.24\n",
      "Iter 113 time=0.06  loss=14299.06 active=36344 feature_norm=194.24\n",
      "Iter 114 time=0.06  loss=14298.55 active=36327 feature_norm=194.26\n",
      "Iter 115 time=0.06  loss=14298.00 active=36315 feature_norm=194.25\n",
      "Iter 116 time=0.06  loss=14296.99 active=36306 feature_norm=194.27\n",
      "Iter 117 time=0.06  loss=14296.37 active=36315 feature_norm=194.27\n",
      "Iter 118 time=0.06  loss=14295.59 active=36309 feature_norm=194.28\n",
      "Iter 119 time=0.06  loss=14295.24 active=36309 feature_norm=194.27\n",
      "Iter 120 time=0.06  loss=14294.51 active=36306 feature_norm=194.29\n",
      "Iter 121 time=0.06  loss=14294.01 active=36309 feature_norm=194.28\n",
      "Iter 122 time=0.06  loss=14293.28 active=36292 feature_norm=194.29\n",
      "Iter 123 time=0.07  loss=14292.90 active=36300 feature_norm=194.29\n",
      "Iter 124 time=0.07  loss=14292.23 active=36283 feature_norm=194.30\n",
      "Iter 125 time=0.07  loss=14291.73 active=36287 feature_norm=194.30\n",
      "Iter 126 time=0.07  loss=14291.30 active=36282 feature_norm=194.31\n",
      "Iter 127 time=0.07  loss=14290.99 active=36283 feature_norm=194.31\n",
      "Iter 128 time=0.07  loss=14290.11 active=36274 feature_norm=194.33\n",
      "Iter 129 time=0.07  loss=14289.74 active=36263 feature_norm=194.32\n",
      "Iter 130 time=0.07  loss=14289.27 active=36249 feature_norm=194.34\n",
      "Iter 131 time=0.07  loss=14288.93 active=36233 feature_norm=194.33\n",
      "Iter 132 time=0.06  loss=14288.33 active=36221 feature_norm=194.34\n",
      "Iter 133 time=0.06  loss=14288.00 active=36232 feature_norm=194.34\n",
      "Iter 134 time=0.06  loss=14287.54 active=36226 feature_norm=194.35\n",
      "Iter 135 time=0.06  loss=14287.25 active=36216 feature_norm=194.34\n",
      "Iter 136 time=0.06  loss=14286.70 active=36210 feature_norm=194.36\n",
      "Iter 137 time=0.06  loss=14286.38 active=36210 feature_norm=194.35\n",
      "Iter 138 time=0.06  loss=14285.85 active=36204 feature_norm=194.36\n",
      "Iter 139 time=0.06  loss=14285.58 active=36195 feature_norm=194.36\n",
      "Iter 140 time=0.06  loss=14285.06 active=36185 feature_norm=194.37\n",
      "Iter 141 time=0.06  loss=14284.88 active=36179 feature_norm=194.36\n",
      "Iter 142 time=0.06  loss=14284.27 active=36174 feature_norm=194.37\n",
      "Iter 143 time=0.06  loss=14284.07 active=36168 feature_norm=194.36\n",
      "Iter 144 time=0.06  loss=14283.47 active=36173 feature_norm=194.37\n",
      "Iter 145 time=0.06  loss=14283.31 active=36166 feature_norm=194.37\n",
      "Iter 146 time=0.07  loss=14282.69 active=36162 feature_norm=194.38\n",
      "Iter 147 time=0.06  loss=14282.62 active=36156 feature_norm=194.38\n",
      "Iter 148 time=0.06  loss=14281.88 active=36145 feature_norm=194.39\n",
      "Iter 149 time=0.06  loss=14281.86 active=36138 feature_norm=194.38\n",
      "Iter 150 time=0.06  loss=14281.17 active=36134 feature_norm=194.40\n",
      "Iter 151 time=0.06  loss=14281.15 active=36131 feature_norm=194.39\n",
      "Iter 152 time=0.06  loss=14280.40 active=36131 feature_norm=194.40\n",
      "Iter 153 time=0.06  loss=14280.09 active=36146 feature_norm=194.40\n",
      "Iter 154 time=0.07  loss=14279.82 active=36132 feature_norm=194.40\n",
      "Iter 155 time=0.06  loss=14279.58 active=36119 feature_norm=194.40\n",
      "Iter 156 time=0.06  loss=14279.02 active=36120 feature_norm=194.40\n",
      "Iter 157 time=0.06  loss=14278.72 active=36124 feature_norm=194.40\n",
      "Iter 158 time=0.06  loss=14278.41 active=36113 feature_norm=194.40\n",
      "Iter 159 time=0.06  loss=14278.16 active=36105 feature_norm=194.40\n",
      "Iter 160 time=0.07  loss=14277.70 active=36085 feature_norm=194.40\n",
      "Iter 161 time=0.06  loss=14277.41 active=36078 feature_norm=194.40\n",
      "Iter 162 time=0.06  loss=14277.06 active=36081 feature_norm=194.41\n",
      "Iter 163 time=0.06  loss=14276.87 active=36083 feature_norm=194.40\n",
      "Iter 164 time=0.06  loss=14276.44 active=36076 feature_norm=194.41\n",
      "Iter 165 time=0.06  loss=14276.27 active=36064 feature_norm=194.40\n",
      "Iter 166 time=0.06  loss=14275.84 active=36063 feature_norm=194.41\n",
      "Iter 167 time=0.07  loss=14275.80 active=36057 feature_norm=194.40\n",
      "Iter 168 time=0.07  loss=14275.34 active=36040 feature_norm=194.41\n",
      "Iter 169 time=0.07  loss=14275.26 active=36034 feature_norm=194.40\n",
      "Iter 170 time=0.07  loss=14274.92 active=36028 feature_norm=194.41\n",
      "Iter 171 time=0.07  loss=14274.87 active=36018 feature_norm=194.41\n",
      "Iter 172 time=0.07  loss=14274.51 active=36017 feature_norm=194.41\n",
      "Iter 173 time=0.06  loss=14274.46 active=36016 feature_norm=194.41\n",
      "Iter 174 time=0.06  loss=14274.08 active=36013 feature_norm=194.42\n",
      "Iter 175 time=0.06  loss=14274.04 active=36015 feature_norm=194.41\n",
      "Iter 176 time=0.06  loss=14273.67 active=36004 feature_norm=194.42\n",
      "Iter 177 time=0.06  loss=14273.63 active=35990 feature_norm=194.41\n",
      "Iter 178 time=0.06  loss=14273.28 active=35988 feature_norm=194.42\n",
      "Iter 179 time=0.07  loss=14273.25 active=35982 feature_norm=194.41\n",
      "Iter 180 time=0.06  loss=14272.90 active=35979 feature_norm=194.42\n",
      "Iter 181 time=0.06  loss=14272.88 active=35982 feature_norm=194.42\n",
      "Iter 182 time=0.06  loss=14272.53 active=35978 feature_norm=194.42\n",
      "Iter 183 time=0.06  loss=14272.50 active=35976 feature_norm=194.42\n",
      "Iter 184 time=0.06  loss=14272.18 active=35970 feature_norm=194.42\n",
      "Iter 185 time=0.06  loss=14272.16 active=35956 feature_norm=194.41\n",
      "Iter 186 time=0.06  loss=14271.85 active=35949 feature_norm=194.42\n",
      "Iter 187 time=0.06  loss=14271.83 active=35941 feature_norm=194.41\n",
      "Iter 188 time=0.06  loss=14271.51 active=35934 feature_norm=194.41\n",
      "Iter 189 time=0.11  loss=14271.38 active=35941 feature_norm=194.41\n",
      "Iter 190 time=0.06  loss=14271.29 active=35933 feature_norm=194.41\n",
      "Iter 191 time=0.06  loss=14271.18 active=35932 feature_norm=194.40\n",
      "Iter 192 time=0.06  loss=14270.82 active=35927 feature_norm=194.40\n",
      "Iter 193 time=0.06  loss=14270.76 active=35930 feature_norm=194.40\n",
      "Iter 194 time=0.06  loss=14270.56 active=35923 feature_norm=194.40\n",
      "Iter 195 time=0.06  loss=14270.45 active=35918 feature_norm=194.39\n",
      "Iter 196 time=0.06  loss=14270.30 active=35915 feature_norm=194.39\n",
      "Iter 197 time=0.06  loss=14270.16 active=35908 feature_norm=194.38\n",
      "Iter 198 time=0.06  loss=14270.01 active=35901 feature_norm=194.38\n",
      "Iter 199 time=0.06  loss=14269.89 active=35900 feature_norm=194.37\n",
      "Iter 200 time=0.06  loss=14269.70 active=35898 feature_norm=194.37\n",
      "Iter 201 time=0.06  loss=14269.61 active=35898 feature_norm=194.36\n",
      "Iter 202 time=0.06  loss=14269.44 active=35899 feature_norm=194.36\n",
      "Iter 203 time=0.06  loss=14269.38 active=35896 feature_norm=194.35\n",
      "Iter 204 time=0.06  loss=14269.21 active=35891 feature_norm=194.35\n",
      "Iter 205 time=0.06  loss=14269.13 active=35887 feature_norm=194.35\n",
      "Iter 206 time=0.06  loss=14268.99 active=35886 feature_norm=194.35\n",
      "Iter 207 time=0.06  loss=14268.90 active=35892 feature_norm=194.34\n",
      "Iter 208 time=0.06  loss=14268.74 active=35889 feature_norm=194.34\n",
      "Iter 209 time=0.06  loss=14268.68 active=35883 feature_norm=194.33\n",
      "Iter 210 time=0.06  loss=14268.52 active=35880 feature_norm=194.33\n",
      "Iter 211 time=0.06  loss=14268.45 active=35878 feature_norm=194.33\n",
      "Iter 212 time=0.06  loss=14268.29 active=35877 feature_norm=194.33\n",
      "Iter 213 time=0.06  loss=14268.26 active=35875 feature_norm=194.32\n",
      "Iter 214 time=0.06  loss=14268.06 active=35871 feature_norm=194.32\n",
      "Iter 215 time=0.06  loss=14268.03 active=35872 feature_norm=194.31\n",
      "Iter 216 time=0.06  loss=14267.84 active=35865 feature_norm=194.31\n",
      "Iter 217 time=0.06  loss=14267.81 active=35859 feature_norm=194.31\n",
      "Iter 218 time=0.06  loss=14267.66 active=35853 feature_norm=194.31\n",
      "Iter 219 time=0.06  loss=14267.63 active=35848 feature_norm=194.30\n",
      "Iter 220 time=0.06  loss=14267.47 active=35848 feature_norm=194.30\n",
      "Iter 221 time=0.06  loss=14267.46 active=35846 feature_norm=194.29\n",
      "Iter 222 time=0.06  loss=14267.30 active=35843 feature_norm=194.29\n",
      "Iter 223 time=0.06  loss=14267.29 active=35848 feature_norm=194.29\n",
      "Iter 224 time=0.06  loss=14267.14 active=35843 feature_norm=194.29\n",
      "Iter 225 time=0.06  loss=14267.13 active=35841 feature_norm=194.28\n",
      "Iter 226 time=0.06  loss=14266.98 active=35839 feature_norm=194.28\n",
      "Iter 227 time=0.06  loss=14266.96 active=35832 feature_norm=194.28\n",
      "Iter 228 time=0.06  loss=14266.83 active=35832 feature_norm=194.28\n",
      "Iter 229 time=0.06  loss=14266.81 active=35836 feature_norm=194.27\n",
      "Iter 230 time=0.06  loss=14266.68 active=35831 feature_norm=194.27\n",
      "Iter 231 time=0.06  loss=14266.66 active=35826 feature_norm=194.27\n",
      "Iter 232 time=0.06  loss=14266.54 active=35820 feature_norm=194.26\n",
      "Iter 233 time=0.06  loss=14266.52 active=35822 feature_norm=194.26\n",
      "Iter 234 time=0.06  loss=14266.41 active=35817 feature_norm=194.26\n",
      "Iter 235 time=0.06  loss=14266.38 active=35811 feature_norm=194.25\n",
      "Iter 236 time=0.06  loss=14266.28 active=35810 feature_norm=194.25\n",
      "Iter 237 time=0.06  loss=14266.25 active=35802 feature_norm=194.25\n",
      "Iter 238 time=0.06  loss=14266.16 active=35801 feature_norm=194.25\n",
      "Iter 239 time=0.06  loss=14266.12 active=35804 feature_norm=194.24\n",
      "Iter 240 time=0.06  loss=14266.03 active=35801 feature_norm=194.24\n",
      "Iter 241 time=0.06  loss=14266.00 active=35801 feature_norm=194.23\n",
      "Iter 242 time=0.06  loss=14265.92 active=35797 feature_norm=194.23\n",
      "Iter 243 time=0.06  loss=14265.88 active=35792 feature_norm=194.22\n",
      "Iter 244 time=0.06  loss=14265.81 active=35787 feature_norm=194.22\n",
      "Iter 245 time=0.06  loss=14265.77 active=35787 feature_norm=194.22\n",
      "Iter 246 time=0.06  loss=14265.71 active=35784 feature_norm=194.22\n",
      "Iter 247 time=0.06  loss=14265.66 active=35778 feature_norm=194.21\n",
      "Iter 248 time=0.06  loss=14265.62 active=35776 feature_norm=194.21\n",
      "Iter 249 time=0.06  loss=14265.56 active=35770 feature_norm=194.20\n",
      "Iter 250 time=0.06  loss=14265.51 active=35771 feature_norm=194.20\n",
      "Iter 251 time=0.06  loss=14265.46 active=35768 feature_norm=194.20\n",
      "Iter 252 time=0.06  loss=14265.41 active=35770 feature_norm=194.19\n",
      "Iter 253 time=0.06  loss=14265.36 active=35773 feature_norm=194.19\n",
      "Iter 254 time=0.06  loss=14265.31 active=35770 feature_norm=194.19\n",
      "Iter 255 time=0.06  loss=14265.27 active=35766 feature_norm=194.18\n",
      "Iter 256 time=0.06  loss=14265.21 active=35759 feature_norm=194.18\n",
      "Iter 257 time=0.06  loss=14265.17 active=35759 feature_norm=194.18\n",
      "Iter 258 time=0.06  loss=14265.12 active=35755 feature_norm=194.18\n",
      "Iter 259 time=0.06  loss=14265.09 active=35755 feature_norm=194.17\n",
      "Iter 260 time=0.06  loss=14265.03 active=35756 feature_norm=194.17\n",
      "Iter 261 time=0.06  loss=14265.00 active=35752 feature_norm=194.17\n",
      "Iter 262 time=0.06  loss=14264.96 active=35749 feature_norm=194.17\n",
      "Iter 263 time=0.06  loss=14264.92 active=35754 feature_norm=194.17\n",
      "Iter 264 time=0.06  loss=14264.88 active=35755 feature_norm=194.17\n",
      "Iter 265 time=0.06  loss=14264.85 active=35750 feature_norm=194.16\n",
      "Iter 266 time=0.06  loss=14264.81 active=35743 feature_norm=194.16\n",
      "Iter 267 time=0.06  loss=14264.78 active=35746 feature_norm=194.16\n",
      "Iter 268 time=0.06  loss=14264.74 active=35749 feature_norm=194.16\n",
      "Iter 269 time=0.06  loss=14264.71 active=35747 feature_norm=194.16\n",
      "Iter 270 time=0.06  loss=14264.66 active=35747 feature_norm=194.16\n",
      "Iter 271 time=0.06  loss=14264.64 active=35738 feature_norm=194.16\n",
      "Iter 272 time=0.06  loss=14264.59 active=35734 feature_norm=194.16\n",
      "Iter 273 time=0.07  loss=14264.56 active=35735 feature_norm=194.15\n",
      "Iter 274 time=0.06  loss=14264.53 active=35736 feature_norm=194.16\n",
      "Iter 275 time=0.06  loss=14264.50 active=35735 feature_norm=194.15\n",
      "Iter 276 time=0.06  loss=14264.47 active=35734 feature_norm=194.15\n",
      "Iter 277 time=0.06  loss=14264.44 active=35731 feature_norm=194.15\n",
      "Iter 278 time=0.06  loss=14264.40 active=35731 feature_norm=194.15\n",
      "Iter 279 time=0.06  loss=14264.38 active=35730 feature_norm=194.15\n",
      "Iter 280 time=0.06  loss=14264.35 active=35729 feature_norm=194.15\n",
      "Iter 281 time=0.06  loss=14264.32 active=35728 feature_norm=194.15\n",
      "Iter 282 time=0.06  loss=14264.28 active=35725 feature_norm=194.15\n",
      "Iter 283 time=0.06  loss=14264.26 active=35722 feature_norm=194.15\n",
      "Iter 284 time=0.06  loss=14264.22 active=35721 feature_norm=194.15\n",
      "Iter 285 time=0.06  loss=14264.20 active=35718 feature_norm=194.14\n",
      "Iter 286 time=0.06  loss=14264.17 active=35715 feature_norm=194.15\n",
      "Iter 287 time=0.06  loss=14264.14 active=35711 feature_norm=194.14\n",
      "Iter 288 time=0.06  loss=14264.10 active=35709 feature_norm=194.15\n",
      "Iter 289 time=0.06  loss=14264.08 active=35713 feature_norm=194.14\n",
      "Iter 290 time=0.06  loss=14264.05 active=35707 feature_norm=194.14\n",
      "Iter 291 time=0.06  loss=14264.02 active=35705 feature_norm=194.14\n",
      "Iter 292 time=0.06  loss=14264.00 active=35702 feature_norm=194.14\n",
      "Iter 293 time=0.06  loss=14263.97 active=35703 feature_norm=194.14\n",
      "Iter 294 time=0.06  loss=14263.95 active=35700 feature_norm=194.14\n",
      "Iter 295 time=0.06  loss=14263.92 active=35689 feature_norm=194.14\n",
      "Iter 296 time=0.06  loss=14263.89 active=35684 feature_norm=194.14\n",
      "Iter 297 time=0.06  loss=14263.86 active=35682 feature_norm=194.14\n",
      "Iter 298 time=0.06  loss=14263.83 active=35676 feature_norm=194.14\n",
      "Iter 299 time=0.06  loss=14263.80 active=35671 feature_norm=194.13\n",
      "Iter 300 time=0.06  loss=14263.77 active=35668 feature_norm=194.14\n",
      "Iter 301 time=0.06  loss=14263.74 active=35673 feature_norm=194.13\n",
      "Iter 302 time=0.06  loss=14263.72 active=35674 feature_norm=194.13\n",
      "Iter 303 time=0.06  loss=14263.69 active=35672 feature_norm=194.13\n",
      "Iter 304 time=0.06  loss=14263.66 active=35664 feature_norm=194.13\n",
      "Iter 305 time=0.06  loss=14263.64 active=35659 feature_norm=194.13\n",
      "Iter 306 time=0.06  loss=14263.62 active=35658 feature_norm=194.13\n",
      "Iter 307 time=0.07  loss=14263.59 active=35656 feature_norm=194.13\n",
      "Iter 308 time=0.06  loss=14263.57 active=35652 feature_norm=194.13\n",
      "Iter 309 time=0.06  loss=14263.55 active=35655 feature_norm=194.13\n",
      "Iter 310 time=0.06  loss=14263.52 active=35653 feature_norm=194.13\n",
      "Iter 311 time=0.06  loss=14263.49 active=35652 feature_norm=194.12\n",
      "Iter 312 time=0.06  loss=14263.48 active=35650 feature_norm=194.13\n",
      "Iter 313 time=0.06  loss=14263.45 active=35650 feature_norm=194.12\n",
      "Iter 314 time=0.06  loss=14263.43 active=35649 feature_norm=194.12\n",
      "Iter 315 time=0.06  loss=14263.40 active=35649 feature_norm=194.12\n",
      "Iter 316 time=0.06  loss=14263.37 active=35647 feature_norm=194.12\n",
      "Iter 317 time=0.06  loss=14263.36 active=35647 feature_norm=194.12\n",
      "Iter 318 time=0.06  loss=14263.33 active=35647 feature_norm=194.12\n",
      "Iter 319 time=0.06  loss=14263.30 active=35645 feature_norm=194.12\n",
      "Iter 320 time=0.06  loss=14263.29 active=35649 feature_norm=194.12\n",
      "Iter 321 time=0.06  loss=14263.25 active=35649 feature_norm=194.12\n",
      "Iter 322 time=0.06  loss=14263.25 active=35647 feature_norm=194.12\n",
      "Iter 323 time=0.06  loss=14263.21 active=35647 feature_norm=194.12\n",
      "Iter 324 time=0.06  loss=14263.19 active=35643 feature_norm=194.12\n",
      "Iter 325 time=0.06  loss=14263.16 active=35645 feature_norm=194.11\n",
      "Iter 326 time=0.06  loss=14263.16 active=35643 feature_norm=194.11\n",
      "Iter 327 time=0.06  loss=14263.12 active=35641 feature_norm=194.11\n",
      "Iter 328 time=0.06  loss=14263.12 active=35642 feature_norm=194.11\n",
      "Iter 329 time=0.06  loss=14263.08 active=35644 feature_norm=194.11\n",
      "Iter 330 time=0.06  loss=14263.07 active=35636 feature_norm=194.11\n",
      "Iter 331 time=0.06  loss=14263.04 active=35638 feature_norm=194.11\n",
      "Iter 332 time=0.11  loss=14263.03 active=35639 feature_norm=194.11\n",
      "Iter 333 time=0.06  loss=14263.01 active=35644 feature_norm=194.11\n",
      "Iter 334 time=0.11  loss=14262.98 active=35643 feature_norm=194.11\n",
      "Iter 335 time=0.06  loss=14262.97 active=35641 feature_norm=194.11\n",
      "Iter 336 time=0.06  loss=14262.95 active=35637 feature_norm=194.11\n",
      "Iter 337 time=0.06  loss=14262.91 active=35639 feature_norm=194.11\n",
      "Iter 338 time=0.06  loss=14262.89 active=35637 feature_norm=194.11\n",
      "Iter 339 time=0.06  loss=14262.87 active=35638 feature_norm=194.11\n",
      "Iter 340 time=0.06  loss=14262.85 active=35638 feature_norm=194.11\n",
      "Iter 341 time=0.06  loss=14262.83 active=35636 feature_norm=194.11\n",
      "Iter 342 time=0.06  loss=14262.81 active=35638 feature_norm=194.11\n",
      "Iter 343 time=0.06  loss=14262.78 active=35643 feature_norm=194.11\n",
      "Iter 344 time=0.06  loss=14262.76 active=35642 feature_norm=194.11\n",
      "Iter 345 time=0.06  loss=14262.74 active=35637 feature_norm=194.11\n",
      "Iter 346 time=0.06  loss=14262.73 active=35639 feature_norm=194.11\n",
      "Iter 347 time=0.06  loss=14262.70 active=35638 feature_norm=194.11\n",
      "Iter 348 time=0.06  loss=14262.69 active=35638 feature_norm=194.11\n",
      "Iter 349 time=0.06  loss=14262.66 active=35641 feature_norm=194.11\n",
      "Iter 350 time=0.06  loss=14262.64 active=35643 feature_norm=194.11\n",
      "Iter 351 time=0.06  loss=14262.62 active=35640 feature_norm=194.11\n",
      "Iter 352 time=0.06  loss=14262.61 active=35638 feature_norm=194.11\n",
      "Iter 353 time=0.06  loss=14262.57 active=35636 feature_norm=194.10\n",
      "Iter 354 time=0.06  loss=14262.57 active=35637 feature_norm=194.11\n",
      "Iter 355 time=0.06  loss=14262.53 active=35631 feature_norm=194.10\n",
      "Iter 356 time=0.06  loss=14262.52 active=35628 feature_norm=194.10\n",
      "Iter 357 time=0.06  loss=14262.50 active=35629 feature_norm=194.10\n",
      "Iter 358 time=0.06  loss=14262.49 active=35629 feature_norm=194.10\n",
      "Iter 359 time=0.06  loss=14262.46 active=35630 feature_norm=194.10\n",
      "Iter 360 time=0.06  loss=14262.45 active=35631 feature_norm=194.10\n",
      "Iter 361 time=0.06  loss=14262.42 active=35634 feature_norm=194.10\n",
      "Iter 362 time=0.06  loss=14262.40 active=35630 feature_norm=194.10\n",
      "Iter 363 time=0.06  loss=14262.39 active=35623 feature_norm=194.10\n",
      "Iter 364 time=0.06  loss=14262.38 active=35621 feature_norm=194.10\n",
      "Iter 365 time=0.06  loss=14262.35 active=35624 feature_norm=194.10\n",
      "Iter 366 time=0.06  loss=14262.34 active=35624 feature_norm=194.10\n",
      "Iter 367 time=0.06  loss=14262.31 active=35629 feature_norm=194.10\n",
      "Iter 368 time=0.06  loss=14262.29 active=35625 feature_norm=194.10\n",
      "Iter 369 time=0.06  loss=14262.27 active=35622 feature_norm=194.10\n",
      "Iter 370 time=0.11  loss=14262.25 active=35621 feature_norm=194.10\n",
      "Iter 371 time=0.11  loss=14262.24 active=35620 feature_norm=194.10\n",
      "Iter 372 time=0.11  loss=14262.23 active=35621 feature_norm=194.10\n",
      "Iter 373 time=0.11  loss=14262.21 active=35621 feature_norm=194.10\n",
      "Iter 374 time=0.11  loss=14262.19 active=35617 feature_norm=194.10\n",
      "Iter 375 time=0.11  loss=14262.17 active=35616 feature_norm=194.10\n",
      "Iter 376 time=0.11  loss=14262.16 active=35617 feature_norm=194.10\n",
      "Iter 377 time=0.12  loss=14262.15 active=35617 feature_norm=194.10\n",
      "Iter 378 time=0.12  loss=14262.14 active=35616 feature_norm=194.10\n",
      "Iter 379 time=0.13  loss=14262.12 active=35616 feature_norm=194.10\n",
      "Iter 380 time=0.13  loss=14262.11 active=35618 feature_norm=194.10\n",
      "Iter 381 time=0.13  loss=14262.09 active=35617 feature_norm=194.10\n",
      "Iter 382 time=0.13  loss=14262.08 active=35618 feature_norm=194.10\n",
      "Iter 383 time=0.12  loss=14262.06 active=35617 feature_norm=194.10\n",
      "Iter 384 time=0.11  loss=14262.05 active=35618 feature_norm=194.10\n",
      "Iter 385 time=0.11  loss=14262.03 active=35613 feature_norm=194.09\n",
      "Iter 386 time=0.11  loss=14262.01 active=35614 feature_norm=194.10\n",
      "Iter 387 time=0.11  loss=14262.00 active=35606 feature_norm=194.09\n",
      "Iter 388 time=0.11  loss=14261.99 active=35604 feature_norm=194.10\n",
      "Iter 389 time=0.11  loss=14261.97 active=35609 feature_norm=194.09\n",
      "Iter 390 time=0.11  loss=14261.96 active=35611 feature_norm=194.09\n",
      "Iter 391 time=0.11  loss=14261.94 active=35612 feature_norm=194.09\n",
      "Iter 392 time=0.11  loss=14261.93 active=35612 feature_norm=194.09\n",
      "Iter 393 time=0.06  loss=14261.93 active=35609 feature_norm=194.09\n",
      "L-BFGS terminated with the stopping criteria\n",
      "Total seconds required for training: 25.407\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 35609 (98972)\n",
      "Number of active attributes: 27906 (86398)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.039\n",
      "\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', #Градиентный спуск с использованием метода L-BFGS\n",
    "    c1=0.1, #Коэффициент для регуляризации L1\n",
    "    c2=0.1, #Коэффициент для регуляризации L2\n",
    "    max_iterations=1000, #Максимальное количество итераций\n",
    "    all_possible_transitions=True, #Генерация объектов (не встречающихся в обучающих данных)\n",
    "    verbose=True #Включение режима тренировки\n",
    ")\n",
    "\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities = sorted(df.entities.unique().tolist())\n",
    "len(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464624149713488"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities) #f1 для элементов последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.94      0.80      0.86      2613\n",
      "         ORG       0.83      0.62      0.71      4260\n",
      "         OUT       0.96      0.99      0.97     66116\n",
      "         PER       0.92      0.79      0.85      4740\n",
      "\n",
      "    accuracy                           0.95     77729\n",
      "   macro avg       0.91      0.80      0.85     77729\n",
      "weighted avg       0.95      0.95      0.95     77729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = all_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Будем передавать в модель только токен**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем данные и метки\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'беженцев',\n",
       " 'word[-3:]': 'цев',\n",
       " 'word[-2:]': 'ев',\n",
       " 'word.isdigit()': False}"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#передаем в датасет только токен\n",
    "X[1][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем данные на train и test\n",
    "X_train = X[:700]\n",
    "X_test = X[700:]\n",
    "y_train = y[:700]\n",
    "y_test = y[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|███████████████████████████████████████████| 700/700 [00:00<00:00, 1118.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 36201\n",
      "Seconds required: 0.161\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 1000\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.08  loss=121565.02 active=36028 feature_norm=1.00\n",
      "Iter 2   time=0.04  loss=109123.73 active=35080 feature_norm=1.60\n",
      "Iter 3   time=0.04  loss=106856.31 active=35907 feature_norm=1.47\n",
      "Iter 4   time=0.04  loss=105891.97 active=35865 feature_norm=1.41\n",
      "Iter 5   time=0.04  loss=104524.09 active=35926 feature_norm=1.45\n",
      "Iter 6   time=0.04  loss=91351.58 active=35827 feature_norm=4.28\n",
      "Iter 7   time=0.04  loss=89575.26 active=36102 feature_norm=4.60\n",
      "Iter 8   time=0.04  loss=86633.70 active=36186 feature_norm=4.98\n",
      "Iter 9   time=0.04  loss=85236.06 active=36160 feature_norm=6.24\n",
      "Iter 10  time=0.04  loss=82786.92 active=36185 feature_norm=6.17\n",
      "Iter 11  time=0.05  loss=81242.71 active=36189 feature_norm=6.73\n",
      "Iter 12  time=0.04  loss=74187.46 active=36162 feature_norm=11.13\n",
      "Iter 13  time=0.04  loss=68906.89 active=36107 feature_norm=14.48\n",
      "Iter 14  time=0.04  loss=65218.03 active=36127 feature_norm=16.87\n",
      "Iter 15  time=0.04  loss=61807.89 active=36144 feature_norm=19.60\n",
      "Iter 16  time=0.04  loss=57700.34 active=35233 feature_norm=24.11\n",
      "Iter 17  time=0.04  loss=53729.69 active=34746 feature_norm=29.19\n",
      "Iter 18  time=0.04  loss=50400.07 active=33002 feature_norm=34.57\n",
      "Iter 19  time=0.04  loss=48080.66 active=31833 feature_norm=42.45\n",
      "Iter 20  time=0.04  loss=45304.24 active=31758 feature_norm=44.85\n",
      "Iter 21  time=0.04  loss=43485.54 active=31417 feature_norm=49.09\n",
      "Iter 22  time=0.04  loss=39974.54 active=30153 feature_norm=61.18\n",
      "Iter 23  time=0.08  loss=39371.07 active=30200 feature_norm=68.38\n",
      "Iter 24  time=0.04  loss=36943.75 active=30144 feature_norm=73.70\n",
      "Iter 25  time=0.04  loss=35122.31 active=29952 feature_norm=81.12\n",
      "Iter 26  time=0.04  loss=32717.24 active=29517 feature_norm=92.71\n",
      "Iter 27  time=0.04  loss=30307.85 active=28986 feature_norm=104.68\n",
      "Iter 28  time=0.04  loss=28352.89 active=28722 feature_norm=117.26\n",
      "Iter 29  time=0.04  loss=26792.28 active=28224 feature_norm=130.20\n",
      "Iter 30  time=0.04  loss=25646.20 active=27722 feature_norm=148.09\n",
      "Iter 31  time=0.04  loss=24663.36 active=27628 feature_norm=154.78\n",
      "Iter 32  time=0.04  loss=23977.58 active=27383 feature_norm=161.19\n",
      "Iter 33  time=0.04  loss=23220.02 active=26714 feature_norm=169.40\n",
      "Iter 34  time=0.12  loss=22980.49 active=26471 feature_norm=169.70\n",
      "Iter 35  time=0.04  loss=22464.42 active=26286 feature_norm=176.09\n",
      "Iter 36  time=0.04  loss=22155.06 active=26211 feature_norm=180.99\n",
      "Iter 37  time=0.04  loss=21846.39 active=26002 feature_norm=188.31\n",
      "Iter 38  time=0.04  loss=21636.41 active=25946 feature_norm=190.17\n",
      "Iter 39  time=0.04  loss=21535.54 active=25790 feature_norm=191.42\n",
      "Iter 40  time=0.04  loss=21533.65 active=24976 feature_norm=198.05\n",
      "Iter 41  time=0.04  loss=21289.45 active=25089 feature_norm=196.88\n",
      "Iter 42  time=0.04  loss=21277.10 active=25117 feature_norm=197.16\n",
      "Iter 43  time=0.04  loss=21263.40 active=24823 feature_norm=199.10\n",
      "Iter 44  time=0.04  loss=21189.44 active=24846 feature_norm=199.47\n",
      "Iter 45  time=0.04  loss=21175.03 active=24828 feature_norm=199.58\n",
      "Iter 46  time=0.04  loss=21143.37 active=24681 feature_norm=200.12\n",
      "Iter 47  time=0.04  loss=21097.25 active=24515 feature_norm=200.81\n",
      "Iter 48  time=0.04  loss=21020.93 active=24285 feature_norm=202.87\n",
      "Iter 49  time=0.04  loss=20981.16 active=24216 feature_norm=203.62\n",
      "Iter 50  time=0.04  loss=20944.78 active=24142 feature_norm=203.95\n",
      "Iter 51  time=0.04  loss=20887.76 active=23504 feature_norm=204.39\n",
      "Iter 52  time=0.09  loss=20869.74 active=23089 feature_norm=204.52\n",
      "Iter 53  time=0.05  loss=20822.23 active=22822 feature_norm=204.84\n",
      "Iter 54  time=0.04  loss=20791.05 active=22644 feature_norm=204.84\n",
      "Iter 55  time=0.08  loss=20766.46 active=22296 feature_norm=203.98\n",
      "Iter 56  time=0.04  loss=20739.53 active=22144 feature_norm=204.01\n",
      "Iter 57  time=0.04  loss=20725.44 active=22147 feature_norm=203.90\n",
      "Iter 58  time=0.04  loss=20704.25 active=22074 feature_norm=203.75\n",
      "Iter 59  time=0.04  loss=20685.14 active=21952 feature_norm=203.63\n",
      "Iter 60  time=0.04  loss=20672.10 active=21783 feature_norm=204.29\n",
      "Iter 61  time=0.04  loss=20646.58 active=21738 feature_norm=203.71\n",
      "Iter 62  time=0.04  loss=20637.62 active=21721 feature_norm=203.78\n",
      "Iter 63  time=0.04  loss=20620.46 active=21590 feature_norm=203.93\n",
      "Iter 64  time=0.04  loss=20610.29 active=21599 feature_norm=203.93\n",
      "Iter 65  time=0.04  loss=20592.04 active=21487 feature_norm=203.79\n",
      "Iter 66  time=0.04  loss=20589.31 active=21420 feature_norm=204.06\n",
      "Iter 67  time=0.04  loss=20571.14 active=21416 feature_norm=204.09\n",
      "Iter 68  time=0.04  loss=20564.77 active=21396 feature_norm=204.09\n",
      "Iter 69  time=0.04  loss=20555.07 active=21325 feature_norm=204.10\n",
      "Iter 70  time=0.04  loss=20546.13 active=21260 feature_norm=204.21\n",
      "Iter 71  time=0.04  loss=20537.22 active=21219 feature_norm=204.19\n",
      "Iter 72  time=0.04  loss=20530.82 active=21165 feature_norm=204.25\n",
      "Iter 73  time=0.04  loss=20521.99 active=21144 feature_norm=204.22\n",
      "Iter 74  time=0.04  loss=20515.92 active=21071 feature_norm=204.29\n",
      "Iter 75  time=0.04  loss=20508.14 active=21033 feature_norm=204.25\n",
      "Iter 76  time=0.04  loss=20502.58 active=20997 feature_norm=204.30\n",
      "Iter 77  time=0.04  loss=20496.98 active=20975 feature_norm=204.27\n",
      "Iter 78  time=0.05  loss=20492.55 active=20955 feature_norm=204.33\n",
      "Iter 79  time=0.04  loss=20487.35 active=20910 feature_norm=204.31\n",
      "Iter 80  time=0.04  loss=20482.98 active=20875 feature_norm=204.36\n",
      "Iter 81  time=0.04  loss=20479.15 active=20875 feature_norm=204.34\n",
      "Iter 82  time=0.04  loss=20474.98 active=20863 feature_norm=204.38\n",
      "Iter 83  time=0.04  loss=20471.81 active=20873 feature_norm=204.36\n",
      "Iter 84  time=0.04  loss=20467.79 active=20839 feature_norm=204.39\n",
      "Iter 85  time=0.04  loss=20464.96 active=20841 feature_norm=204.37\n",
      "Iter 86  time=0.05  loss=20461.29 active=20814 feature_norm=204.43\n",
      "Iter 87  time=0.04  loss=20458.46 active=20809 feature_norm=204.43\n",
      "Iter 88  time=0.04  loss=20454.84 active=20792 feature_norm=204.49\n",
      "Iter 89  time=0.04  loss=20452.60 active=20778 feature_norm=204.50\n",
      "Iter 90  time=0.04  loss=20450.02 active=20766 feature_norm=204.52\n",
      "Iter 91  time=0.04  loss=20448.28 active=20776 feature_norm=204.52\n",
      "Iter 92  time=0.04  loss=20445.56 active=20763 feature_norm=204.56\n",
      "Iter 93  time=0.04  loss=20444.16 active=20764 feature_norm=204.56\n",
      "Iter 94  time=0.04  loss=20442.23 active=20747 feature_norm=204.59\n",
      "Iter 95  time=0.04  loss=20440.61 active=20744 feature_norm=204.60\n",
      "Iter 96  time=0.04  loss=20438.66 active=20723 feature_norm=204.63\n",
      "Iter 97  time=0.04  loss=20437.56 active=20726 feature_norm=204.62\n",
      "Iter 98  time=0.04  loss=20435.72 active=20718 feature_norm=204.65\n",
      "Iter 99  time=0.04  loss=20434.67 active=20712 feature_norm=204.64\n",
      "Iter 100 time=0.04  loss=20432.90 active=20709 feature_norm=204.66\n",
      "Iter 101 time=0.04  loss=20431.77 active=20724 feature_norm=204.66\n",
      "Iter 102 time=0.04  loss=20430.33 active=20722 feature_norm=204.68\n",
      "Iter 103 time=0.04  loss=20429.42 active=20733 feature_norm=204.69\n",
      "Iter 104 time=0.04  loss=20428.00 active=20727 feature_norm=204.70\n",
      "Iter 105 time=0.04  loss=20427.07 active=20733 feature_norm=204.70\n",
      "Iter 106 time=0.04  loss=20425.93 active=20721 feature_norm=204.71\n",
      "Iter 107 time=0.04  loss=20424.98 active=20734 feature_norm=204.71\n",
      "Iter 108 time=0.04  loss=20423.71 active=20727 feature_norm=204.72\n",
      "Iter 109 time=0.04  loss=20422.94 active=20730 feature_norm=204.72\n",
      "Iter 110 time=0.04  loss=20421.58 active=20721 feature_norm=204.73\n",
      "Iter 111 time=0.04  loss=20420.72 active=20723 feature_norm=204.73\n",
      "Iter 112 time=0.04  loss=20419.52 active=20715 feature_norm=204.74\n",
      "Iter 113 time=0.04  loss=20418.61 active=20721 feature_norm=204.74\n",
      "Iter 114 time=0.04  loss=20417.52 active=20716 feature_norm=204.74\n",
      "Iter 115 time=0.04  loss=20416.69 active=20724 feature_norm=204.74\n",
      "Iter 116 time=0.04  loss=20415.58 active=20714 feature_norm=204.74\n",
      "Iter 117 time=0.04  loss=20414.87 active=20710 feature_norm=204.73\n",
      "Iter 118 time=0.04  loss=20413.46 active=20709 feature_norm=204.73\n",
      "Iter 119 time=0.04  loss=20412.75 active=20723 feature_norm=204.72\n",
      "Iter 120 time=0.04  loss=20411.53 active=20718 feature_norm=204.72\n",
      "Iter 121 time=0.04  loss=20410.78 active=20723 feature_norm=204.71\n",
      "Iter 122 time=0.04  loss=20409.60 active=20716 feature_norm=204.71\n",
      "Iter 123 time=0.04  loss=20408.82 active=20722 feature_norm=204.69\n",
      "Iter 124 time=0.04  loss=20407.73 active=20716 feature_norm=204.69\n",
      "Iter 125 time=0.04  loss=20407.02 active=20717 feature_norm=204.67\n",
      "Iter 126 time=0.04  loss=20406.00 active=20706 feature_norm=204.66\n",
      "Iter 127 time=0.04  loss=20405.34 active=20704 feature_norm=204.64\n",
      "Iter 128 time=0.04  loss=20404.23 active=20703 feature_norm=204.64\n",
      "Iter 129 time=0.04  loss=20403.56 active=20699 feature_norm=204.61\n",
      "Iter 130 time=0.04  loss=20402.57 active=20695 feature_norm=204.60\n",
      "Iter 131 time=0.04  loss=20401.93 active=20697 feature_norm=204.58\n",
      "Iter 132 time=0.04  loss=20400.77 active=20688 feature_norm=204.57\n",
      "Iter 133 time=0.04  loss=20400.14 active=20682 feature_norm=204.55\n",
      "Iter 134 time=0.04  loss=20399.19 active=20670 feature_norm=204.54\n",
      "Iter 135 time=0.04  loss=20398.57 active=20669 feature_norm=204.52\n",
      "Iter 136 time=0.04  loss=20397.59 active=20661 feature_norm=204.51\n",
      "Iter 137 time=0.04  loss=20397.01 active=20663 feature_norm=204.49\n",
      "Iter 138 time=0.04  loss=20395.93 active=20657 feature_norm=204.48\n",
      "Iter 139 time=0.04  loss=20395.45 active=20657 feature_norm=204.45\n",
      "Iter 140 time=0.04  loss=20394.38 active=20656 feature_norm=204.44\n",
      "Iter 141 time=0.04  loss=20393.75 active=20678 feature_norm=204.41\n",
      "Iter 142 time=0.04  loss=20392.91 active=20677 feature_norm=204.41\n",
      "Iter 143 time=0.04  loss=20392.37 active=20685 feature_norm=204.38\n",
      "Iter 144 time=0.04  loss=20391.58 active=20679 feature_norm=204.38\n",
      "Iter 145 time=0.04  loss=20391.11 active=20671 feature_norm=204.35\n",
      "Iter 146 time=0.04  loss=20390.35 active=20673 feature_norm=204.35\n",
      "Iter 147 time=0.04  loss=20389.90 active=20689 feature_norm=204.33\n",
      "Iter 148 time=0.04  loss=20389.19 active=20685 feature_norm=204.32\n",
      "Iter 149 time=0.04  loss=20388.75 active=20705 feature_norm=204.30\n",
      "Iter 150 time=0.04  loss=20388.11 active=20694 feature_norm=204.29\n",
      "Iter 151 time=0.04  loss=20387.74 active=20708 feature_norm=204.27\n",
      "Iter 152 time=0.04  loss=20387.14 active=20706 feature_norm=204.27\n",
      "Iter 153 time=0.04  loss=20386.71 active=20718 feature_norm=204.25\n",
      "Iter 154 time=0.04  loss=20386.20 active=20711 feature_norm=204.25\n",
      "Iter 155 time=0.04  loss=20385.80 active=20708 feature_norm=204.23\n",
      "Iter 156 time=0.04  loss=20385.31 active=20701 feature_norm=204.23\n",
      "Iter 157 time=0.04  loss=20384.97 active=20716 feature_norm=204.22\n",
      "Iter 158 time=0.04  loss=20384.50 active=20714 feature_norm=204.21\n",
      "Iter 159 time=0.04  loss=20384.15 active=20724 feature_norm=204.20\n",
      "Iter 160 time=0.04  loss=20383.72 active=20723 feature_norm=204.20\n",
      "Iter 161 time=0.04  loss=20383.40 active=20729 feature_norm=204.19\n",
      "Iter 162 time=0.04  loss=20382.87 active=20724 feature_norm=204.18\n",
      "Iter 163 time=0.04  loss=20382.59 active=20731 feature_norm=204.17\n",
      "Iter 164 time=0.04  loss=20382.04 active=20726 feature_norm=204.16\n",
      "Iter 165 time=0.04  loss=20381.74 active=20727 feature_norm=204.14\n",
      "Iter 166 time=0.04  loss=20381.17 active=20723 feature_norm=204.14\n",
      "Iter 167 time=0.04  loss=20380.87 active=20725 feature_norm=204.12\n",
      "Iter 168 time=0.04  loss=20380.37 active=20713 feature_norm=204.12\n",
      "Iter 169 time=0.04  loss=20380.05 active=20720 feature_norm=204.10\n",
      "Iter 170 time=0.04  loss=20379.54 active=20718 feature_norm=204.09\n",
      "Iter 171 time=0.04  loss=20379.16 active=20731 feature_norm=204.08\n",
      "Iter 172 time=0.04  loss=20378.69 active=20728 feature_norm=204.07\n",
      "Iter 173 time=0.04  loss=20378.35 active=20728 feature_norm=204.05\n",
      "Iter 174 time=0.04  loss=20377.87 active=20728 feature_norm=204.04\n",
      "Iter 175 time=0.04  loss=20377.54 active=20729 feature_norm=204.03\n",
      "Iter 176 time=0.04  loss=20377.08 active=20727 feature_norm=204.02\n",
      "Iter 177 time=0.04  loss=20376.74 active=20733 feature_norm=204.01\n",
      "Iter 178 time=0.04  loss=20376.35 active=20724 feature_norm=204.00\n",
      "Iter 179 time=0.04  loss=20376.04 active=20725 feature_norm=203.99\n",
      "Iter 180 time=0.04  loss=20375.75 active=20721 feature_norm=203.98\n",
      "Iter 181 time=0.04  loss=20375.43 active=20728 feature_norm=203.97\n",
      "Iter 182 time=0.04  loss=20375.15 active=20728 feature_norm=203.96\n",
      "Iter 183 time=0.04  loss=20374.82 active=20729 feature_norm=203.95\n",
      "Iter 184 time=0.04  loss=20374.56 active=20731 feature_norm=203.94\n",
      "Iter 185 time=0.05  loss=20374.20 active=20739 feature_norm=203.93\n",
      "Iter 186 time=0.05  loss=20373.91 active=20738 feature_norm=203.93\n",
      "Iter 187 time=0.05  loss=20373.60 active=20742 feature_norm=203.92\n",
      "Iter 188 time=0.05  loss=20373.32 active=20736 feature_norm=203.91\n",
      "Iter 189 time=0.04  loss=20372.97 active=20751 feature_norm=203.90\n",
      "Iter 190 time=0.04  loss=20372.70 active=20749 feature_norm=203.90\n",
      "Iter 191 time=0.04  loss=20372.43 active=20780 feature_norm=203.89\n",
      "Iter 192 time=0.04  loss=20372.16 active=20776 feature_norm=203.88\n",
      "Iter 193 time=0.04  loss=20371.87 active=20779 feature_norm=203.88\n",
      "Iter 194 time=0.04  loss=20371.61 active=20780 feature_norm=203.87\n",
      "Iter 195 time=0.04  loss=20371.39 active=20779 feature_norm=203.86\n",
      "Iter 196 time=0.04  loss=20371.11 active=20769 feature_norm=203.86\n",
      "Iter 197 time=0.04  loss=20370.89 active=20772 feature_norm=203.85\n",
      "Iter 198 time=0.04  loss=20370.62 active=20772 feature_norm=203.85\n",
      "Iter 199 time=0.04  loss=20370.40 active=20775 feature_norm=203.84\n",
      "Iter 200 time=0.04  loss=20370.20 active=20770 feature_norm=203.84\n",
      "Iter 201 time=0.04  loss=20369.97 active=20773 feature_norm=203.83\n",
      "Iter 202 time=0.04  loss=20369.77 active=20773 feature_norm=203.83\n",
      "Iter 203 time=0.04  loss=20369.58 active=20776 feature_norm=203.82\n",
      "Iter 204 time=0.04  loss=20369.41 active=20775 feature_norm=203.82\n",
      "Iter 205 time=0.04  loss=20369.21 active=20773 feature_norm=203.82\n",
      "Iter 206 time=0.04  loss=20369.01 active=20768 feature_norm=203.82\n",
      "Iter 207 time=0.04  loss=20368.88 active=20766 feature_norm=203.81\n",
      "Iter 208 time=0.04  loss=20368.70 active=20761 feature_norm=203.81\n",
      "Iter 209 time=0.04  loss=20368.50 active=20756 feature_norm=203.80\n",
      "Iter 210 time=0.04  loss=20368.32 active=20756 feature_norm=203.80\n",
      "Iter 211 time=0.04  loss=20368.17 active=20748 feature_norm=203.79\n",
      "Iter 212 time=0.04  loss=20367.98 active=20747 feature_norm=203.79\n",
      "Iter 213 time=0.04  loss=20367.81 active=20747 feature_norm=203.79\n",
      "Iter 214 time=0.04  loss=20367.61 active=20743 feature_norm=203.79\n",
      "Iter 215 time=0.04  loss=20367.48 active=20742 feature_norm=203.78\n",
      "Iter 216 time=0.04  loss=20367.30 active=20740 feature_norm=203.78\n",
      "Iter 217 time=0.04  loss=20367.14 active=20738 feature_norm=203.77\n",
      "Iter 218 time=0.04  loss=20366.96 active=20736 feature_norm=203.77\n",
      "Iter 219 time=0.04  loss=20366.79 active=20733 feature_norm=203.77\n",
      "Iter 220 time=0.04  loss=20366.61 active=20728 feature_norm=203.77\n",
      "Iter 221 time=0.04  loss=20366.43 active=20725 feature_norm=203.76\n",
      "Iter 222 time=0.04  loss=20366.24 active=20723 feature_norm=203.76\n",
      "Iter 223 time=0.04  loss=20366.05 active=20720 feature_norm=203.75\n",
      "Iter 224 time=0.04  loss=20365.84 active=20712 feature_norm=203.75\n",
      "Iter 225 time=0.04  loss=20365.64 active=20709 feature_norm=203.74\n",
      "Iter 226 time=0.04  loss=20365.45 active=20707 feature_norm=203.74\n",
      "Iter 227 time=0.04  loss=20365.27 active=20706 feature_norm=203.73\n",
      "Iter 228 time=0.04  loss=20365.10 active=20703 feature_norm=203.73\n",
      "Iter 229 time=0.04  loss=20364.89 active=20701 feature_norm=203.72\n",
      "Iter 230 time=0.04  loss=20364.70 active=20701 feature_norm=203.72\n",
      "Iter 231 time=0.04  loss=20364.49 active=20702 feature_norm=203.71\n",
      "Iter 232 time=0.04  loss=20364.38 active=20704 feature_norm=203.71\n",
      "Iter 233 time=0.04  loss=20364.16 active=20704 feature_norm=203.71\n",
      "Iter 234 time=0.04  loss=20364.04 active=20703 feature_norm=203.70\n",
      "Iter 235 time=0.04  loss=20363.85 active=20704 feature_norm=203.70\n",
      "Iter 236 time=0.04  loss=20363.77 active=20702 feature_norm=203.70\n",
      "Iter 237 time=0.04  loss=20363.52 active=20700 feature_norm=203.69\n",
      "Iter 238 time=0.04  loss=20363.43 active=20702 feature_norm=203.69\n",
      "Iter 239 time=0.04  loss=20363.24 active=20701 feature_norm=203.69\n",
      "Iter 240 time=0.04  loss=20363.21 active=20697 feature_norm=203.68\n",
      "Iter 241 time=0.04  loss=20362.96 active=20701 feature_norm=203.68\n",
      "Iter 242 time=0.04  loss=20362.91 active=20698 feature_norm=203.68\n",
      "Iter 243 time=0.04  loss=20362.68 active=20695 feature_norm=203.67\n",
      "Iter 244 time=0.04  loss=20362.50 active=20691 feature_norm=203.67\n",
      "Iter 245 time=0.04  loss=20362.42 active=20692 feature_norm=203.66\n",
      "Iter 246 time=0.04  loss=20362.24 active=20692 feature_norm=203.66\n",
      "Iter 247 time=0.04  loss=20362.04 active=20693 feature_norm=203.66\n",
      "Iter 248 time=0.04  loss=20361.91 active=20686 feature_norm=203.65\n",
      "Iter 249 time=0.04  loss=20361.88 active=20683 feature_norm=203.65\n",
      "Iter 250 time=0.04  loss=20361.57 active=20688 feature_norm=203.65\n",
      "Iter 251 time=0.04  loss=20361.53 active=20688 feature_norm=203.64\n",
      "Iter 252 time=0.04  loss=20361.35 active=20688 feature_norm=203.64\n",
      "Iter 253 time=0.04  loss=20361.25 active=20689 feature_norm=203.64\n",
      "Iter 254 time=0.04  loss=20361.10 active=20687 feature_norm=203.64\n",
      "Iter 255 time=0.04  loss=20360.99 active=20682 feature_norm=203.63\n",
      "Iter 256 time=0.04  loss=20360.96 active=20686 feature_norm=203.63\n",
      "Iter 257 time=0.04  loss=20360.82 active=20685 feature_norm=203.63\n",
      "Iter 258 time=0.04  loss=20360.68 active=20686 feature_norm=203.63\n",
      "Iter 259 time=0.04  loss=20360.51 active=20682 feature_norm=203.63\n",
      "Iter 260 time=0.04  loss=20360.42 active=20674 feature_norm=203.62\n",
      "Iter 261 time=0.04  loss=20360.25 active=20675 feature_norm=203.61\n",
      "Iter 262 time=0.04  loss=20360.10 active=20677 feature_norm=203.61\n",
      "Iter 263 time=0.04  loss=20359.92 active=20681 feature_norm=203.61\n",
      "Iter 264 time=0.04  loss=20359.83 active=20680 feature_norm=203.60\n",
      "Iter 265 time=0.04  loss=20359.81 active=20679 feature_norm=203.60\n",
      "Iter 266 time=0.04  loss=20359.61 active=20678 feature_norm=203.60\n",
      "Iter 267 time=0.05  loss=20359.51 active=20676 feature_norm=203.59\n",
      "Iter 268 time=0.04  loss=20359.43 active=20674 feature_norm=203.59\n",
      "Iter 269 time=0.04  loss=20359.33 active=20673 feature_norm=203.59\n",
      "Iter 270 time=0.04  loss=20359.30 active=20674 feature_norm=203.59\n",
      "Iter 271 time=0.04  loss=20359.20 active=20673 feature_norm=203.59\n",
      "Iter 272 time=0.04  loss=20359.14 active=20670 feature_norm=203.59\n",
      "Iter 273 time=0.04  loss=20359.02 active=20670 feature_norm=203.58\n",
      "Iter 274 time=0.04  loss=20358.95 active=20668 feature_norm=203.58\n",
      "Iter 275 time=0.04  loss=20358.84 active=20669 feature_norm=203.58\n",
      "Iter 276 time=0.04  loss=20358.81 active=20668 feature_norm=203.58\n",
      "Iter 277 time=0.04  loss=20358.66 active=20668 feature_norm=203.57\n",
      "Iter 278 time=0.04  loss=20358.61 active=20663 feature_norm=203.57\n",
      "Iter 279 time=0.04  loss=20358.48 active=20663 feature_norm=203.57\n",
      "Iter 280 time=0.04  loss=20358.41 active=20662 feature_norm=203.57\n",
      "Iter 281 time=0.04  loss=20358.29 active=20663 feature_norm=203.56\n",
      "Iter 282 time=0.04  loss=20358.22 active=20660 feature_norm=203.56\n",
      "Iter 283 time=0.04  loss=20358.14 active=20660 feature_norm=203.56\n",
      "Iter 284 time=0.04  loss=20358.11 active=20657 feature_norm=203.56\n",
      "Iter 285 time=0.04  loss=20357.98 active=20657 feature_norm=203.55\n",
      "Iter 286 time=0.04  loss=20357.95 active=20651 feature_norm=203.55\n",
      "Iter 287 time=0.04  loss=20357.85 active=20650 feature_norm=203.55\n",
      "Iter 288 time=0.04  loss=20357.81 active=20650 feature_norm=203.55\n",
      "Iter 289 time=0.04  loss=20357.73 active=20649 feature_norm=203.55\n",
      "Iter 290 time=0.04  loss=20357.71 active=20646 feature_norm=203.55\n",
      "Iter 291 time=0.04  loss=20357.60 active=20649 feature_norm=203.55\n",
      "Iter 292 time=0.05  loss=20357.58 active=20647 feature_norm=203.55\n",
      "Iter 293 time=0.04  loss=20357.49 active=20645 feature_norm=203.54\n",
      "Iter 294 time=0.04  loss=20357.47 active=20646 feature_norm=203.54\n",
      "Iter 295 time=0.04  loss=20357.36 active=20644 feature_norm=203.54\n",
      "Iter 296 time=0.04  loss=20357.30 active=20641 feature_norm=203.54\n",
      "Iter 297 time=0.05  loss=20357.24 active=20643 feature_norm=203.54\n",
      "Iter 298 time=0.09  loss=20357.17 active=20643 feature_norm=203.54\n",
      "Iter 299 time=0.09  loss=20357.13 active=20643 feature_norm=203.54\n",
      "Iter 300 time=0.04  loss=20357.12 active=20644 feature_norm=203.53\n",
      "Iter 301 time=0.04  loss=20357.00 active=20644 feature_norm=203.53\n",
      "Iter 302 time=0.04  loss=20356.98 active=20643 feature_norm=203.53\n",
      "Iter 303 time=0.04  loss=20356.88 active=20643 feature_norm=203.53\n",
      "Iter 304 time=0.04  loss=20356.88 active=20641 feature_norm=203.53\n",
      "Iter 305 time=0.04  loss=20356.77 active=20641 feature_norm=203.53\n",
      "Iter 306 time=0.04  loss=20356.74 active=20640 feature_norm=203.53\n",
      "Iter 307 time=0.04  loss=20356.68 active=20641 feature_norm=203.53\n",
      "Iter 308 time=0.04  loss=20356.65 active=20639 feature_norm=203.53\n",
      "Iter 309 time=0.04  loss=20356.58 active=20637 feature_norm=203.53\n",
      "Iter 310 time=0.04  loss=20356.55 active=20637 feature_norm=203.53\n",
      "Iter 311 time=0.04  loss=20356.50 active=20638 feature_norm=203.53\n",
      "Iter 312 time=0.04  loss=20356.47 active=20639 feature_norm=203.53\n",
      "Iter 313 time=0.04  loss=20356.41 active=20638 feature_norm=203.53\n",
      "Iter 314 time=0.04  loss=20356.39 active=20638 feature_norm=203.53\n",
      "Iter 315 time=0.04  loss=20356.32 active=20636 feature_norm=203.53\n",
      "Iter 316 time=0.04  loss=20356.31 active=20632 feature_norm=203.53\n",
      "Iter 317 time=0.04  loss=20356.24 active=20630 feature_norm=203.53\n",
      "Iter 318 time=0.04  loss=20356.23 active=20631 feature_norm=203.53\n",
      "Iter 319 time=0.04  loss=20356.15 active=20630 feature_norm=203.53\n",
      "Iter 320 time=0.08  loss=20356.11 active=20630 feature_norm=203.53\n",
      "Iter 321 time=0.04  loss=20356.06 active=20628 feature_norm=203.52\n",
      "Iter 322 time=0.08  loss=20356.01 active=20626 feature_norm=203.52\n",
      "Iter 323 time=0.08  loss=20355.98 active=20626 feature_norm=203.52\n",
      "Iter 324 time=0.08  loss=20355.96 active=20624 feature_norm=203.52\n",
      "Iter 325 time=0.08  loss=20355.90 active=20624 feature_norm=203.52\n",
      "Iter 326 time=0.09  loss=20355.87 active=20620 feature_norm=203.52\n",
      "Iter 327 time=0.05  loss=20355.85 active=20615 feature_norm=203.52\n",
      "Iter 328 time=0.04  loss=20355.74 active=20617 feature_norm=203.52\n",
      "Iter 329 time=0.04  loss=20355.69 active=20617 feature_norm=203.52\n",
      "Iter 330 time=0.05  loss=20355.62 active=20618 feature_norm=203.52\n",
      "Iter 331 time=0.05  loss=20355.56 active=20619 feature_norm=203.52\n",
      "Iter 332 time=0.05  loss=20355.51 active=20616 feature_norm=203.52\n",
      "Iter 333 time=0.05  loss=20355.45 active=20616 feature_norm=203.51\n",
      "Iter 334 time=0.04  loss=20355.42 active=20614 feature_norm=203.51\n",
      "Iter 335 time=0.05  loss=20355.36 active=20609 feature_norm=203.51\n",
      "Iter 336 time=0.05  loss=20355.33 active=20610 feature_norm=203.51\n",
      "Iter 337 time=0.05  loss=20355.28 active=20609 feature_norm=203.51\n",
      "Iter 338 time=0.04  loss=20355.24 active=20606 feature_norm=203.51\n",
      "Iter 339 time=0.04  loss=20355.22 active=20606 feature_norm=203.51\n",
      "Iter 340 time=0.04  loss=20355.18 active=20602 feature_norm=203.51\n",
      "Iter 341 time=0.04  loss=20355.13 active=20602 feature_norm=203.51\n",
      "Iter 342 time=0.04  loss=20355.11 active=20601 feature_norm=203.51\n",
      "Iter 343 time=0.04  loss=20355.05 active=20602 feature_norm=203.51\n",
      "Iter 344 time=0.04  loss=20355.04 active=20603 feature_norm=203.51\n",
      "Iter 345 time=0.04  loss=20354.99 active=20602 feature_norm=203.51\n",
      "Iter 346 time=0.04  loss=20354.98 active=20603 feature_norm=203.51\n",
      "Iter 347 time=0.04  loss=20354.92 active=20602 feature_norm=203.50\n",
      "Iter 348 time=0.04  loss=20354.91 active=20602 feature_norm=203.50\n",
      "Iter 349 time=0.04  loss=20354.86 active=20599 feature_norm=203.50\n",
      "Iter 350 time=0.04  loss=20354.85 active=20600 feature_norm=203.50\n",
      "Iter 351 time=0.04  loss=20354.81 active=20600 feature_norm=203.50\n",
      "Iter 352 time=0.04  loss=20354.79 active=20603 feature_norm=203.50\n",
      "Iter 353 time=0.04  loss=20354.75 active=20602 feature_norm=203.50\n",
      "Iter 354 time=0.04  loss=20354.74 active=20600 feature_norm=203.50\n",
      "Iter 355 time=0.04  loss=20354.69 active=20602 feature_norm=203.50\n",
      "Iter 356 time=0.04  loss=20354.68 active=20602 feature_norm=203.50\n",
      "Iter 357 time=0.04  loss=20354.63 active=20600 feature_norm=203.50\n",
      "Iter 358 time=0.04  loss=20354.62 active=20593 feature_norm=203.50\n",
      "Iter 359 time=0.04  loss=20354.56 active=20594 feature_norm=203.50\n",
      "Iter 360 time=0.04  loss=20354.56 active=20592 feature_norm=203.50\n",
      "Iter 361 time=0.04  loss=20354.50 active=20592 feature_norm=203.50\n",
      "Iter 362 time=0.04  loss=20354.49 active=20591 feature_norm=203.50\n",
      "Iter 363 time=0.04  loss=20354.43 active=20594 feature_norm=203.50\n",
      "Iter 364 time=0.04  loss=20354.42 active=20595 feature_norm=203.50\n",
      "Iter 365 time=0.04  loss=20354.37 active=20595 feature_norm=203.50\n",
      "Iter 366 time=0.04  loss=20354.36 active=20596 feature_norm=203.50\n",
      "Iter 367 time=0.04  loss=20354.30 active=20597 feature_norm=203.50\n",
      "Iter 368 time=0.04  loss=20354.30 active=20599 feature_norm=203.50\n",
      "Iter 369 time=0.04  loss=20354.24 active=20596 feature_norm=203.50\n",
      "Iter 370 time=0.04  loss=20354.23 active=20592 feature_norm=203.50\n",
      "Iter 371 time=0.04  loss=20354.18 active=20592 feature_norm=203.50\n",
      "Iter 372 time=0.04  loss=20354.16 active=20592 feature_norm=203.50\n",
      "Iter 373 time=0.04  loss=20354.12 active=20592 feature_norm=203.50\n",
      "Iter 374 time=0.04  loss=20354.11 active=20590 feature_norm=203.50\n",
      "Iter 375 time=0.04  loss=20354.07 active=20590 feature_norm=203.49\n",
      "Iter 376 time=0.04  loss=20354.05 active=20587 feature_norm=203.50\n",
      "Iter 377 time=0.04  loss=20354.01 active=20588 feature_norm=203.49\n",
      "Iter 378 time=0.04  loss=20353.99 active=20588 feature_norm=203.50\n",
      "Iter 379 time=0.04  loss=20353.95 active=20590 feature_norm=203.50\n",
      "Iter 380 time=0.04  loss=20353.92 active=20590 feature_norm=203.50\n",
      "Iter 381 time=0.04  loss=20353.90 active=20588 feature_norm=203.50\n",
      "Iter 382 time=0.04  loss=20353.87 active=20589 feature_norm=203.50\n",
      "Iter 383 time=0.04  loss=20353.85 active=20592 feature_norm=203.50\n",
      "Iter 384 time=0.04  loss=20353.83 active=20593 feature_norm=203.50\n",
      "Iter 385 time=0.04  loss=20353.80 active=20591 feature_norm=203.50\n",
      "Iter 386 time=0.04  loss=20353.78 active=20587 feature_norm=203.50\n",
      "Iter 387 time=0.04  loss=20353.76 active=20587 feature_norm=203.50\n",
      "Iter 388 time=0.04  loss=20353.73 active=20588 feature_norm=203.50\n",
      "Iter 389 time=0.04  loss=20353.71 active=20587 feature_norm=203.50\n",
      "Iter 390 time=0.04  loss=20353.68 active=20588 feature_norm=203.50\n",
      "Iter 391 time=0.04  loss=20353.66 active=20586 feature_norm=203.50\n",
      "Iter 392 time=0.04  loss=20353.64 active=20585 feature_norm=203.50\n",
      "Iter 393 time=0.04  loss=20353.61 active=20585 feature_norm=203.50\n",
      "Iter 394 time=0.04  loss=20353.59 active=20583 feature_norm=203.50\n",
      "Iter 395 time=0.04  loss=20353.57 active=20579 feature_norm=203.50\n",
      "Iter 396 time=0.04  loss=20353.55 active=20579 feature_norm=203.50\n",
      "Iter 397 time=0.04  loss=20353.52 active=20579 feature_norm=203.50\n",
      "Iter 398 time=0.04  loss=20353.51 active=20579 feature_norm=203.50\n",
      "Iter 399 time=0.04  loss=20353.48 active=20579 feature_norm=203.50\n",
      "Iter 400 time=0.04  loss=20353.47 active=20573 feature_norm=203.50\n",
      "Iter 401 time=0.04  loss=20353.44 active=20571 feature_norm=203.50\n",
      "Iter 402 time=0.04  loss=20353.44 active=20569 feature_norm=203.50\n",
      "L-BFGS terminated with the stopping criteria\n",
      "Total seconds required for training: 17.348\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 20569 (36201)\n",
      "Number of active attributes: 17654 (32585)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.025\n",
      "\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=1000,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities = sorted(df.entities.unique().tolist())\n",
    "len(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408612727110087"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.92      0.76      0.84      2613\n",
      "         ORG       0.82      0.55      0.66      4260\n",
      "         OUT       0.95      0.99      0.97     66116\n",
      "         PER       0.92      0.79      0.85      4740\n",
      "\n",
      "    accuracy                           0.94     77729\n",
      "   macro avg       0.90      0.77      0.83     77729\n",
      "weighted avg       0.94      0.94      0.94     77729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = all_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** передача в модель токенов и его соседей дает чуть более лучший результат и работает чуть дольше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Выводы <a id='section_2.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовые решения NER из spacy, slovnet показывают хорошие результаты в задачах извлечения именованных сущностей (NER из nltk справляется с задачей хуже). При составлении собственного алгоритма NER необходимо обращать внимание на контекст: передавать в алгоритм не только токен, но и его ближайших соседей (например, предыдущий и последующий токены)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
